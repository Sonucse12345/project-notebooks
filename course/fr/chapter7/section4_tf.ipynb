{"cells":[{"cell_type":"markdown","metadata":{"id":"lQwwCPeVK7lU"},"source":["# Traduction (TensorFlow)"]},{"cell_type":"markdown","metadata":{"id":"jID8fKanK7lX"},"source":["Installez les biblioth√®ques ü§ó *Datasets* et ü§ó *Transformers* pour ex√©cuter ce *notebook*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_9ZNCn0K7lZ"},"outputs":[],"source":["!pip install datasets transformers[sentencepiece]\n","!apt install git-lfs"]},{"cell_type":"markdown","metadata":{"id":"WxFtihYXK7lc"},"source":["Vous aurez besoin de configurer git, adaptez votre email et votre nom dans la cellule suivante."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"woP6GDVCK7le"},"outputs":[],"source":["!git config --global user.email \"you@example.com\"\n","!git config --global user.name \"Your Name\""]},{"cell_type":"markdown","metadata":{"id":"MLMwx807K7lf"},"source":["Vous devrez √©galement √™tre connect√© au Hub d'Hugging Face. Ex√©cutez ce qui suit et entrez vos informations d'identification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3P9UXPJsK7lf"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0YziRB-K7lg"},"outputs":[],"source":["from datasets import load_dataset, load_metric\n","\n","raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFCg2ntAK7li"},"outputs":[],"source":["raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V09kRyLyK7ll"},"outputs":[],"source":["split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n","split_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvvrBODbK7lm"},"outputs":[],"source":["split_datasets[\"validation\"] = split_datasets.pop(\"test\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GROMRExK7ln"},"outputs":[],"source":["split_datasets[\"train\"][1][\"translation\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpAXVL-EK7lo"},"outputs":[],"source":["from transformers import pipeline\n","\n","model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n","translator = pipeline(\"translation\", model=model_checkpoint)\n","translator(\"Default to expanded threads\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epLNmY_HK7lp"},"outputs":[],"source":["split_datasets[\"train\"][172][\"translation\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bY-XZVJCK7lp"},"outputs":[],"source":["translator(\n","    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGpi3GlmK7lq"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"tf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wtw_yJF2K7lr"},"outputs":[],"source":["en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n","fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n","\n","inputs = tokenizer(en_sentence)\n","with tokenizer.as_target_tokenizer():\n","    targets = tokenizer(fr_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZV7SMUWK7ls"},"outputs":[],"source":["wrong_targets = tokenizer(fr_sentence)\n","print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n","print(tokenizer.convert_ids_to_tokens(targets[\"input_ids\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2YcgzcaK7lt"},"outputs":[],"source":["max_input_length = 128\n","max_target_length = 128\n","\n","\n","def preprocess_function(examples):\n","    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n","    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","\n","    # Configurer le tokenizer pour les cibles\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRpke7NdK7lt"},"outputs":[],"source":["tokenized_datasets = split_datasets.map(\n","    preprocess_function,\n","    batched=True,\n","    remove_columns=split_datasets[\"train\"].column_names,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Me0u8VVvK7lu"},"outputs":[],"source":["from transformers import TFAutoModelForSeq2SeqLM\n","\n","model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6lZL_LWK7lu"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uf9FgnBFK7lv"},"outputs":[],"source":["batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n","batch.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Axr6br0pK7lv"},"outputs":[],"source":["batch[\"labels\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nw1IhmWRK7lw"},"outputs":[],"source":["batch[\"decoder_input_ids\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0Q725XgK7lw"},"outputs":[],"source":["for i in range(1, 3):\n","    print(tokenized_datasets[\"train\"][i][\"labels\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8uOxCo-K7ly"},"outputs":[],"source":["tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    collate_fn=data_collator,\n","    shuffle=True,\n","    batch_size=32,\n",")\n","tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n","    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","    collate_fn=data_collator,\n","    shuffle=False,\n","    batch_size=16,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6SFfWWHK7ly"},"outputs":[],"source":["!pip install sacrebleu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDadfs1JK7ly"},"outputs":[],"source":["from datasets import load_metric\n","\n","metric = load_metric(\"sacrebleu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9Moa_SuK7lz"},"outputs":[],"source":["predictions = [\n","    \"This plugin lets you translate web pages between several languages automatically.\"\n","]\n","references = [\n","    [\n","        \"This plugin allows you to automatically translate web pages between several languages.\"\n","    ]\n","]\n","metric.compute(predictions=predictions, references=references)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozUUl2TxK7lz"},"outputs":[],"source":["predictions = [\"This This This This\"]\n","references = [\n","    [\n","        \"This plugin allows you to automatically translate web pages between several languages.\"\n","    ]\n","]\n","metric.compute(predictions=predictions, references=references)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dszazssQK7l0"},"outputs":[],"source":["predictions = [\"This plugin\"]\n","references = [\n","    [\n","        \"This plugin allows you to automatically translate web pages between several languages.\"\n","    ]\n","]\n","metric.compute(predictions=predictions, references=references)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmHOtSV4K7l0"},"outputs":[],"source":["import numpy as np\n","\n","\n","def compute_metrics():\n","    all_preds = []\n","    all_labels = []\n","    sampled_dataset = tokenized_datasets[\"validation\"].shuffle().select(range(200))\n","    tf_generate_dataset = sampled_dataset.to_tf_dataset(\n","        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n","        collate_fn=data_collator,\n","        shuffle=False,\n","        batch_size=4,\n","    )\n","    for batch in tf_generate_dataset:\n","        predictions = model.generate(\n","            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n","        )\n","        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","        labels = batch[\"labels\"].numpy()\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = [pred.strip() for pred in decoded_preds]\n","        decoded_labels = [[label.strip()] for label in decoded_labels]\n","        all_preds.extend(decoded_preds)\n","        all_labels.extend(decoded_labels)\n","\n","    result = metric.compute(predictions=all_preds, references=all_labels)\n","    return {\"bleu\": result[\"score\"]}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X11nbLUtK7l1"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmSjXUvBK7l2"},"outputs":[],"source":["print(compute_metrics())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7ciXHiMK7l2"},"outputs":[],"source":["from transformers import create_optimizer\n","from transformers.keras_callbacks import PushToHubCallback\n","import tensorflow as tf\n","\n","# Le nombre d'√©tapes d'entra√Ænement est le nombre d'√©chantillons dans le jeu de donn√©es, divis√© par la taille du batch puis multipli√©\n","# par le nombre total d'√©poques. Notez que le jeu de donn√©es tf_train_dataset est ici un lot de donn√©es tf.data.Dataset,\n","# pas le jeu de donn√©es original Hugging Face, donc son len() est d√©j√† num_samples // batch_size.\n","num_epochs = 3\n","num_train_steps = len(tf_train_dataset) * num_epochs\n","\n","optimizer, schedule = create_optimizer(\n","    init_lr=5e-5,\n","    num_warmup_steps=0,\n","    num_train_steps=num_train_steps,\n","    weight_decay_rate=0.01,\n",")\n","model.compile(optimizer=optimizer)\n","\n","# Entra√Æner en mixed-precision float16\n","tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zpd9e2R1K7l3"},"outputs":[],"source":["from transformers.keras_callbacks import PushToHubCallback\n","\n","callback = PushToHubCallback(\n","    output_dir=\"marian-finetuned-kde4-en-to-fr\", tokenizer=tokenizer\n",")\n","\n","model.fit(\n","    tf_train_dataset,\n","    validation_data=tf_eval_dataset,\n","    callbacks=[callback],\n","    epochs=num_epochs,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGGvxYbUK7l3"},"outputs":[],"source":["print(compute_metrics())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XBfYvd5qK7l4"},"outputs":[],"source":["from transformers import pipeline\n","\n","# Remplacer par votre propre checkpoint\n","model_checkpoint = \"huggingface-course/marian-finetuned-kde4-en-to-fr\"\n","translator = pipeline(\"translation\", model=model_checkpoint)\n","translator(\"Default to expanded threads\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18mI4K4PK7l5"},"outputs":[],"source":["translator(\n","    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",")"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}