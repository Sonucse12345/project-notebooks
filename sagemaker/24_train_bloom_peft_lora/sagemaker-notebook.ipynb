{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Large Language Model training with LoRA and Hugging Face\n",
    "\n",
    "In this sagemaker example, we are going to learn how to apply [Low-Rank Adaptation of Large Language Models (LoRA)](https://arxiv.org/abs/2106.09685) to fine-tune BLOOMZ (7 billion parameter version instruction tuned version of BLOOM) on a single GPU. We are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune BLOOM with LoRA and bnb int-8 on Amazon SageMaker\n",
    "4. Deploy the model to Amazon SageMaker Endpoint\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tunin\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- LoRA:¬†[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning:¬†[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning:¬†[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning:¬†[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.26.0\" \"datasets[s3]==2.9.0\" sagemaker py7zr --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "we will use the¬†[samsum](https://huggingface.co/datasets/samsum)¬†dataset, a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"id\": \"13818513\",\n",
    "  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load the¬†`samsum`¬†dataset, we use the¬†`load_dataset()`¬†method from the ü§ó Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/home/ubuntu/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"samsum\", split=\"train\")\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset)}\")\n",
    "# Train dataset size: 14732"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we need to convert our inputs (text) to token IDs. This is done by a ü§ó Transformers Tokenizer. If you are not sure what this means, check out¬†**[chapter 6](https://huggingface.co/course/chapter6/1?fw=tf)**¬†of the Hugging Face Course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id=\"bigscience/bloomz-7b1\"\n",
    "\n",
    "# Load tokenizer of BLOOMZ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 2048 # overwrite wrong value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start training, we need to preprocess our data. Abstractive Summarization is a text-generation task. Our model will take a text as input and generate a summary as output. We want to understand how long our input and output will take to batch our data efficiently.\n",
    "\n",
    "We defined a `prompt_template` which we will use to construct an instruct prompt for better performance of our model. Our `prompt_template` has a ‚Äúfixed‚Äù start and end, and our document is in the middle. This means we need to ensure that the ‚Äúfixed‚Äù template parts + document are not exceeding the max length of the model. \n",
    "We preprocess our dataset before training and save it to disk to then upload it to S3. You could run this step on your local machine or a CPU and upload it to the [Hugging Face Hub](https://huggingface.co/docs/hub/datasets-overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893f4bb7763b488ca5692f0b0923b3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14732 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the chat dialogue:\n",
      "Lydia: Barbecue in one hour?\n",
      "Olivier: Sure!\n",
      "Lydia: Excellent, just bring some beer\n",
      "Olivier: Ok\n",
      "---\n",
      "Summary:\n",
      "Olivier will come to Lydia for barbecue in one hour. He'll bring beer. </s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25235be9d0c40bfb66fd909e28c8ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5006941c35e048f9bd95b3bdba2ad24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1289\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "# custom instruct prompt start\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n{{summary}}{{eos_token}}\"\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = prompt_template.format(dialogue=sample[\"dialogue\"],\n",
    "                                            summary=sample[\"summary\"],\n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29452289fa954f5fa90a753db1d6b92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1289 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-558105141721/processed/samsum-sagemaker/train\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/samsum-sagemaker/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune BLOOM with LoRA and bnb int-8 on Amazon SageMaker\n",
    "\n",
    "In addition to the LoRA technique, we will use [bitsanbytes LLM.int8()](https://huggingface.co/blog/hf-bitsandbytes-integration) to quantize out frozen LLM to int8. This allows us to reduce the needed memory for BLOOMZ ~4x.  \n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements uses PEFT to train our model. If you are interested in how this works check-out [Efficient Large Language Model training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft) blog, where we explain the training script in detail. T\n",
    "\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-peft-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                                # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training', # path where sagemaker will save training dataset\n",
    "  # 'epochs': 3,                                         # number of training epochs\n",
    "  \"max_train_samples\": 50,\n",
    "  'per_device_train_batch_size': 1,                    # batch size for training\n",
    "  'lr': 2e-4,                                          # learning rate used during training\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.2xlarge', # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-06 14:21:10 Starting - Starting the training job...\n",
      "2023-04-06 14:21:25 Starting - Preparing the instances for training.........\n",
      "2023-04-06 14:22:58 Downloading - Downloading input data...\n",
      "2023-04-06 14:23:28 Training - Downloading the training image......\n",
      "2023-04-06 14:24:58 Training - Training image download completed. Training in progress.....bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-04-06 14:25:22,226 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-04-06 14:25:22,243 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-06 14:25:22,252 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-04-06 14:25:22,254 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-04-06 14:25:22,441 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.9 -m pip install -r requirements.txt\n",
      "Collecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 1))\n",
      "Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-5zjgxgvz\n",
      "Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-5zjgxgvz\n",
      "Resolved https://github.com/huggingface/peft.git to commit 382b178911edff38c1ff619bbac2ba556bd2276b\n",
      "Installing build dependencies: started\n",
      "Installing build dependencies: finished with status 'done'\n",
      "Getting requirements to build wheel: started\n",
      "Getting requirements to build wheel: finished with status 'done'\n",
      "Preparing metadata (pyproject.toml): started\n",
      "Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting transformers==4.27.1\n",
      "Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.7/6.7 MB 90.2 MB/s eta 0:00:00\n",
      "Collecting accelerate==0.17.1\n",
      "Downloading accelerate-0.17.1-py3-none-any.whl (212 kB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 212.8/212.8 kB 43.8 MB/s eta 0:00:00\n",
      "Collecting bitsandbytes==0.37.1\n",
      "Downloading bitsandbytes-0.37.1-py3-none-any.whl (76.3 MB)\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 76.3/76.3 MB 38.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.27.1->-r requirements.txt (line 2)) (3.9.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.17.1->-r requirements.txt (line 3)) (1.13.1+cu117)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.1->-r requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.27.1->-r requirements.txt (line 2)) (1.26.14)\n",
      "Building wheels for collected packages: peft\n",
      "Building wheel for peft (pyproject.toml): started\n",
      "Building wheel for peft (pyproject.toml): finished with status 'done'\n",
      "Created wheel for peft: filename=peft-0.3.0.dev0-py3-none-any.whl size=47431 sha256=f288ece2a4f0fbed8a1c0fb41e1a765e62f75a1bc94f1f92523aa6e2e565b3ef\n",
      "Stored in directory: /tmp/pip-ephem-wheel-cache-a67tc3s8/wheels/2d/60/1b/0edd9dc0f0c489738b1166bc1b0b560ee368f7721f89d06e3a\n",
      "Successfully built peft\n",
      "Installing collected packages: bitsandbytes, accelerate, transformers, peft\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.16.0\n",
      "Uninstalling accelerate-0.16.0:\n",
      "Successfully uninstalled accelerate-0.16.0\n",
      "Attempting uninstall: transformers\n",
      "Found existing installation: transformers 4.26.0\n",
      "Uninstalling transformers-4.26.0:\n",
      "Successfully uninstalled transformers-4.26.0\n",
      "Successfully installed accelerate-0.17.1 bitsandbytes-0.37.1 peft-0.3.0.dev0 transformers-4.27.1\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2023-04-06 14:25:36,237 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-04-06 14:25:36,237 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-04-06 14:25:36,258 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-06 14:25:36,286 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-06 14:25:36,313 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-04-06 14:25:36,324 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"lr\": 0.0002,\n",
      "        \"max_train_samples\": 50,\n",
      "        \"model_id\": \"bigscience/bloomz-7b1\",\n",
      "        \"per_device_train_batch_size\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-558105141721/huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"lr\":0.0002,\"max_train_samples\":50,\"model_id\":\"bigscience/bloomz-7b1\",\"per_device_train_batch_size\":1}\n",
      "SM_USER_ENTRY_POINT=run_clm.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"training\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=run_clm\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-558105141721/huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"lr\":0.0002,\"max_train_samples\":50,\"model_id\":\"bigscience/bloomz-7b1\",\"per_device_train_batch_size\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-558105141721/huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\n",
      "SM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--lr\",\"0.0002\",\"--max_train_samples\",\"50\",\"--model_id\",\"bigscience/bloomz-7b1\",\"--per_device_train_batch_size\",\"1\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "SM_HP_DATASET_PATH=/opt/ml/input/data/training\n",
      "SM_HP_LR=0.0002\n",
      "SM_HP_MAX_TRAIN_SAMPLES=50\n",
      "SM_HP_MODEL_ID=bigscience/bloomz-7b1\n",
      "SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=1\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.9 run_clm.py --dataset_path /opt/ml/input/data/training --lr 0.0002 --max_train_samples 50 --model_id bigscience/bloomz-7b1 --per_device_train_batch_size 1\n",
      "[2023-04-06 14:25:37.900: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "2023-04-06 14:25:37,904 root         INFO     Using NamedTuple = typing._NamedTuple instead.\n",
      "2023-04-06 14:25:37,924 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /opt/conda/lib/python3.9/site-packages/smdistributed/dataparallel/lib:/opt/amazon/openmpi/lib/:/opt/amazon/efa/lib/:/opt/conda/lib:/usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('aws'), PosixPath('training-job/huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841'), PosixPath('arn'), PosixPath('558105141721'), PosixPath('sagemaker'), PosixPath('us-east-1')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/v2/credentials/proxy-e1be91375244f167effb77f33ed350f86ca0c67744760a73f4147842a731318f-customer')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('s3'), PosixPath('//sagemaker-us-east-1-558105141721/huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841/source/sourcedir.tar.gz')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('bigscience/bloomz-7b1')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"homogeneousCluster\",\"current_host\"'), PosixPath('50,\"model_id\"'), PosixPath('{\"training\"'), PosixPath('\"/opt/ml/model\",\"module_dir\"'), PosixPath('1,\"num_neurons\"'), PosixPath('\"/opt/ml/input/data/training\"},\"current_host\"'), PosixPath('main\",\"hosts\"'), PosixPath('true,\"job_name\"'), PosixPath('\"algo-1\",\"current_instance_type\"'), PosixPath('\"bigscience/bloomz-7b1\",\"per_device_train_batch_size\"'), PosixPath('20,\"master_hostname\"'), PosixPath('\"FullyReplicated\",\"TrainingInputMode\"'), PosixPath('\"algo-1\",\"model_dir\"'), PosixPath('\"s3'), PosixPath('\"run_clm\",\"network_interface_name\"'), PosixPath('\"File\"}},\"input_dir\"'), PosixPath('0.0002,\"max_train_samples\"'), PosixPath('[\"homogeneousCluster\"],\"instance_groups_dict\"'), PosixPath('\"/opt/ml/input/data/training\",\"lr\"'), PosixPath('\"algo-1\",\"current_instance_group\"'), PosixPath('\"/opt/ml/input\",\"instance_groups\"'), PosixPath('\"ml.g5.2xlarge\",\"hosts\"'), PosixPath('\"eth0\"},\"user_entry_point\"'), PosixPath('\"ml.g5.2xlarge\"}],\"network_interface_name\"'), PosixPath('//sagemaker-us-east-1-558105141721/huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841/source/sourcedir.tar.gz\",\"module_name\"'), PosixPath('[],\"distribution_instance_groups\"'), PosixPath('\"eth0\",\"num_cpus\"'), PosixPath('[\"algo-1\"],\"instance_groups\"'), PosixPath('{\"dataset_path\"'), PosixPath('\"ml.g5.2xlarge\"}},\"is_hetero\"'), PosixPath('\"None\",\"S3DistributionType\"'), PosixPath('{\"homogeneousCluster\"'), PosixPath('\"sagemaker_pytorch_container.training'), PosixPath('[\"algo-1\"],\"hyperparameters\"'), PosixPath('[\"algo-1\"],\"current_instance_type\"'), PosixPath('{},\"channel_input_dirs\"'), PosixPath('1},\"input_config_dir\"'), PosixPath('{\"RecordWrapperType\"'), PosixPath('null,\"is_smddpmprun_installed\"'), PosixPath('\"huggingface-peft-2023-04-06-14-21-08-2023-04-06-14-21-08-841\",\"log_level\"'), PosixPath('0,\"output_data_dir\"'), PosixPath('{\"hosts\"'), PosixPath('\"homogeneousCluster\",\"current_instance_group_hosts\"'), PosixPath('[],\"framework_module\"'), PosixPath('\"/opt/ml/input/config\",\"input_data_config\"'), PosixPath('8,\"num_gpus\"'), PosixPath('{\"additional_framework_parameters\"'), PosixPath('[\"algo-1\"],\"instance_group_name\"'), PosixPath('\"/opt/ml/output/data\",\"output_dir\"'), PosixPath('true,\"is_modelparallel_enabled\"'), PosixPath('false,\"is_master\"'), PosixPath('{\"current_group_name\"'), PosixPath('\"homogeneousCluster\",\"instance_type\"'), PosixPath('\"/opt/ml/output/intermediate\",\"resource_config\"'), PosixPath('\"run_clm.py\"}'), PosixPath('\"/opt/ml/output\",\"output_intermediate_dir\"'), PosixPath('\"ml.g5.2xlarge\",\"distribution_hosts\"'), PosixPath('[{\"hosts\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('50,\"model_id\"'), PosixPath('\"bigscience/bloomz-7b1\",\"per_device_train_batch_size\"'), PosixPath('0.0002,\"max_train_samples\"'), PosixPath('1}'), PosixPath('\"/opt/ml/input/data/training\",\"lr\"'), PosixPath('{\"dataset_path\"')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/ml/output/intermediate')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/conda/lib/python39.zip')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--lr\",\"0.0002\",\"--max_train_samples\",\"50\",\"--model_id\",\"bigscience/bloomz-7b1\",\"--per_device_train_batch_size\",\"1\"]')}\n",
      "  warn(msg)\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('$(dirname $(which conda))/..')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n",
      "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]\n",
      "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 747/747 [00:00<00:00, 220kB/s]\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 0.00/14.1G [00:00<?, ?B/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   0%|          | 41.9M/14.1G [00:00<00:42, 335MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   1%|          | 83.9M/14.1G [00:00<00:49, 286MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   1%|          | 136M/14.1G [00:00<00:43, 319MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   1%|‚ñè         | 178M/14.1G [00:00<00:42, 329MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   2%|‚ñè         | 231M/14.1G [00:00<00:38, 361MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   2%|‚ñè         | 273M/14.1G [00:00<00:48, 289MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   2%|‚ñè         | 315M/14.1G [00:00<00:44, 313MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   3%|‚ñé         | 367M/14.1G [00:01<00:41, 333MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   3%|‚ñé         | 409M/14.1G [00:01<00:45, 303MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   3%|‚ñé         | 451M/14.1G [00:01<00:43, 312MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   3%|‚ñé         | 493M/14.1G [00:01<00:42, 321MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   4%|‚ñç         | 535M/14.1G [00:01<00:46, 295MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   4%|‚ñç         | 566M/14.1G [00:01<00:49, 275MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   4%|‚ñç         | 598M/14.1G [00:01<00:47, 284MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   5%|‚ñç         | 640M/14.1G [00:02<00:43, 311MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   5%|‚ñç         | 682M/14.1G [00:02<00:43, 309MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   5%|‚ñå         | 724M/14.1G [00:02<00:50, 266MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   5%|‚ñå         | 755M/14.1G [00:02<00:49, 272MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   6%|‚ñå         | 797M/14.1G [00:02<00:47, 282MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   6%|‚ñå         | 828M/14.1G [00:02<00:46, 289MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   6%|‚ñå         | 870M/14.1G [00:02<00:45, 293MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   6%|‚ñã         | 902M/14.1G [00:03<00:46, 283MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   7%|‚ñã         | 933M/14.1G [00:03<00:45, 291MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   7%|‚ñã         | 965M/14.1G [00:03<00:45, 289MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   7%|‚ñã         | 996M/14.1G [00:03<00:45, 291MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   7%|‚ñã         | 1.04G/14.1G [00:03<00:42, 309MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   8%|‚ñä         | 1.07G/14.1G [00:03<00:42, 304MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   8%|‚ñä         | 1.11G/14.1G [00:03<00:43, 301MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   8%|‚ñä         | 1.14G/14.1G [00:03<00:45, 287MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   8%|‚ñä         | 1.18G/14.1G [00:03<00:42, 308MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   9%|‚ñä         | 1.23G/14.1G [00:04<00:40, 321MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   9%|‚ñâ         | 1.28G/14.1G [00:04<00:36, 351MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:   9%|‚ñâ         | 1.32G/14.1G [00:04<00:36, 353MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  10%|‚ñâ         | 1.36G/14.1G [00:04<00:36, 352MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  10%|‚ñà         | 1.42G/14.1G [00:04<00:33, 381MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  10%|‚ñà         | 1.46G/14.1G [00:04<00:34, 371MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  11%|‚ñà         | 1.50G/14.1G [00:04<00:39, 322MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  11%|‚ñà         | 1.54G/14.1G [00:04<00:40, 312MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  11%|‚ñà         | 1.58G/14.1G [00:05<00:41, 300MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  11%|‚ñà‚ñè        | 1.63G/14.1G [00:05<00:39, 314MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  12%|‚ñà‚ñè        | 1.67G/14.1G [00:05<00:44, 283MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  12%|‚ñà‚ñè        | 1.71G/14.1G [00:05<00:41, 303MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  12%|‚ñà‚ñè        | 1.75G/14.1G [00:05<00:39, 314MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  13%|‚ñà‚ñé        | 1.79G/14.1G [00:05<00:41, 298MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  13%|‚ñà‚ñé        | 1.82G/14.1G [00:05<00:41, 300MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  13%|‚ñà‚ñé        | 1.87G/14.1G [00:06<00:39, 312MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  13%|‚ñà‚ñé        | 1.91G/14.1G [00:06<00:40, 302MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  14%|‚ñà‚ñé        | 1.94G/14.1G [00:06<00:40, 301MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  14%|‚ñà‚ñç        | 1.98G/14.1G [00:06<00:39, 305MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  14%|‚ñà‚ñç        | 2.02G/14.1G [00:06<00:36, 332MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  15%|‚ñà‚ñç        | 2.07G/14.1G [00:06<00:36, 331MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  15%|‚ñà‚ñç        | 2.12G/14.1G [00:06<00:33, 358MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  15%|‚ñà‚ñå        | 2.16G/14.1G [00:06<00:35, 335MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  16%|‚ñà‚ñå        | 2.20G/14.1G [00:07<00:43, 277MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  16%|‚ñà‚ñå        | 2.23G/14.1G [00:07<00:42, 279MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  16%|‚ñà‚ñå        | 2.28G/14.1G [00:07<00:40, 296MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  16%|‚ñà‚ñã        | 2.32G/14.1G [00:07<00:36, 322MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  17%|‚ñà‚ñã        | 2.36G/14.1G [00:07<00:37, 314MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  17%|‚ñà‚ñã        | 2.40G/14.1G [00:07<00:36, 317MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  17%|‚ñà‚ñã        | 2.44G/14.1G [00:07<00:36, 319MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  18%|‚ñà‚ñä        | 2.49G/14.1G [00:08<00:35, 332MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  18%|‚ñà‚ñä        | 2.53G/14.1G [00:08<00:35, 330MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  18%|‚ñà‚ñä        | 2.57G/14.1G [00:08<00:36, 316MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  18%|‚ñà‚ñä        | 2.61G/14.1G [00:08<00:41, 280MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  19%|‚ñà‚ñä        | 2.64G/14.1G [00:08<00:47, 240MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  19%|‚ñà‚ñâ        | 2.67G/14.1G [00:08<00:51, 222MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  19%|‚ñà‚ñâ        | 2.71G/14.1G [00:09<00:54, 211MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  19%|‚ñà‚ñâ        | 2.74G/14.1G [00:09<00:53, 215MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  20%|‚ñà‚ñâ        | 2.77G/14.1G [00:09<00:49, 231MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  20%|‚ñà‚ñâ        | 2.81G/14.1G [00:09<00:43, 261MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  20%|‚ñà‚ñà        | 2.85G/14.1G [00:09<00:39, 287MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  20%|‚ñà‚ñà        | 2.89G/14.1G [00:09<00:37, 301MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  21%|‚ñà‚ñà        | 2.95G/14.1G [00:09<00:33, 331MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  21%|‚ñà‚ñà        | 2.99G/14.1G [00:09<00:32, 348MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  21%|‚ñà‚ñà‚ñè       | 3.03G/14.1G [00:10<00:33, 330MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  22%|‚ñà‚ñà‚ñè       | 3.07G/14.1G [00:10<00:33, 330MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  22%|‚ñà‚ñà‚ñè       | 3.11G/14.1G [00:10<00:32, 337MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  22%|‚ñà‚ñà‚ñè       | 3.16G/14.1G [00:10<00:33, 332MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  23%|‚ñà‚ñà‚ñé       | 3.20G/14.1G [00:10<00:32, 340MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  23%|‚ñà‚ñà‚ñé       | 3.24G/14.1G [00:10<00:32, 336MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  23%|‚ñà‚ñà‚ñé       | 3.28G/14.1G [00:10<00:49, 221MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  23%|‚ñà‚ñà‚ñé       | 3.31G/14.1G [00:11<00:46, 234MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  24%|‚ñà‚ñà‚ñé       | 3.36G/14.1G [00:11<00:41, 261MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  24%|‚ñà‚ñà‚ñç       | 3.41G/14.1G [00:11<00:34, 311MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  24%|‚ñà‚ñà‚ñç       | 3.46G/14.1G [00:11<00:31, 341MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  25%|‚ñà‚ñà‚ñç       | 3.50G/14.1G [00:11<00:30, 353MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  25%|‚ñà‚ñà‚ñå       | 3.54G/14.1G [00:11<00:29, 355MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  25%|‚ñà‚ñà‚ñå       | 3.59G/14.1G [00:11<00:34, 307MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  26%|‚ñà‚ñà‚ñå       | 3.63G/14.1G [00:11<00:32, 320MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  26%|‚ñà‚ñà‚ñå       | 3.67G/14.1G [00:12<00:31, 327MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  26%|‚ñà‚ñà‚ñã       | 3.71G/14.1G [00:12<00:32, 321MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  27%|‚ñà‚ñà‚ñã       | 3.75G/14.1G [00:12<00:31, 332MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  27%|‚ñà‚ñà‚ñã       | 3.80G/14.1G [00:12<00:32, 322MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  27%|‚ñà‚ñà‚ñã       | 3.84G/14.1G [00:12<00:30, 338MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  27%|‚ñà‚ñà‚ñã       | 3.88G/14.1G [00:12<00:29, 351MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  28%|‚ñà‚ñà‚ñä       | 3.93G/14.1G [00:12<00:27, 374MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  28%|‚ñà‚ñà‚ñä       | 3.97G/14.1G [00:12<00:27, 365MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  28%|‚ñà‚ñà‚ñä       | 4.02G/14.1G [00:13<00:27, 369MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  29%|‚ñà‚ñà‚ñä       | 4.06G/14.1G [00:13<00:28, 352MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  29%|‚ñà‚ñà‚ñâ       | 4.10G/14.1G [00:13<00:28, 358MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  29%|‚ñà‚ñà‚ñâ       | 4.15G/14.1G [00:13<00:25, 388MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  30%|‚ñà‚ñà‚ñâ       | 4.19G/14.1G [00:13<00:25, 393MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  30%|‚ñà‚ñà‚ñâ       | 4.24G/14.1G [00:13<00:25, 386MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  30%|‚ñà‚ñà‚ñà       | 4.28G/14.1G [00:13<00:30, 326MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  31%|‚ñà‚ñà‚ñà       | 4.32G/14.1G [00:13<00:31, 308MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  31%|‚ñà‚ñà‚ñà       | 4.36G/14.1G [00:14<00:32, 302MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  31%|‚ñà‚ñà‚ñà       | 4.40G/14.1G [00:14<00:30, 314MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  31%|‚ñà‚ñà‚ñà‚ñè      | 4.45G/14.1G [00:14<00:30, 316MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  32%|‚ñà‚ñà‚ñà‚ñè      | 4.50G/14.1G [00:14<00:27, 350MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  32%|‚ñà‚ñà‚ñà‚ñè      | 4.54G/14.1G [00:14<00:26, 357MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  32%|‚ñà‚ñà‚ñà‚ñè      | 4.58G/14.1G [00:14<00:26, 358MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  33%|‚ñà‚ñà‚ñà‚ñé      | 4.62G/14.1G [00:14<00:26, 360MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  33%|‚ñà‚ñà‚ñà‚ñé      | 4.67G/14.1G [00:14<00:25, 370MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  33%|‚ñà‚ñà‚ñà‚ñé      | 4.71G/14.1G [00:15<00:25, 365MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  34%|‚ñà‚ñà‚ñà‚ñé      | 4.75G/14.1G [00:15<00:26, 351MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  34%|‚ñà‚ñà‚ñà‚ñç      | 4.79G/14.1G [00:15<00:25, 365MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  34%|‚ñà‚ñà‚ñà‚ñç      | 4.83G/14.1G [00:15<00:26, 345MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  34%|‚ñà‚ñà‚ñà‚ñç      | 4.88G/14.1G [00:15<00:40, 228MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  35%|‚ñà‚ñà‚ñà‚ñç      | 4.93G/14.1G [00:15<00:33, 275MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  35%|‚ñà‚ñà‚ñà‚ñå      | 4.97G/14.1G [00:15<00:32, 278MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  36%|‚ñà‚ñà‚ñà‚ñå      | 5.02G/14.1G [00:16<00:29, 305MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  36%|‚ñà‚ñà‚ñà‚ñå      | 5.06G/14.1G [00:16<00:27, 324MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  36%|‚ñà‚ñà‚ñà‚ñå      | 5.11G/14.1G [00:16<00:26, 336MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  36%|‚ñà‚ñà‚ñà‚ñã      | 5.15G/14.1G [00:16<00:26, 335MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  37%|‚ñà‚ñà‚ñà‚ñã      | 5.19G/14.1G [00:16<00:26, 343MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  37%|‚ñà‚ñà‚ñà‚ñã      | 5.23G/14.1G [00:16<00:25, 349MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  37%|‚ñà‚ñà‚ñà‚ñã      | 5.28G/14.1G [00:16<00:23, 369MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  38%|‚ñà‚ñà‚ñà‚ñä      | 5.33G/14.1G [00:16<00:25, 346MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  38%|‚ñà‚ñà‚ñà‚ñä      | 5.37G/14.1G [00:17<00:24, 354MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  38%|‚ñà‚ñà‚ñà‚ñä      | 5.41G/14.1G [00:17<00:23, 365MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  39%|‚ñà‚ñà‚ñà‚ñä      | 5.45G/14.1G [00:17<00:23, 372MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  39%|‚ñà‚ñà‚ñà‚ñâ      | 5.49G/14.1G [00:17<00:24, 351MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  39%|‚ñà‚ñà‚ñà‚ñâ      | 5.54G/14.1G [00:17<00:24, 351MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  39%|‚ñà‚ñà‚ñà‚ñâ      | 5.58G/14.1G [00:17<00:26, 327MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  40%|‚ñà‚ñà‚ñà‚ñâ      | 5.62G/14.1G [00:17<00:29, 291MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  40%|‚ñà‚ñà‚ñà‚ñâ      | 5.65G/14.1G [00:18<00:30, 277MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  40%|‚ñà‚ñà‚ñà‚ñà      | 5.68G/14.1G [00:18<00:32, 263MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  40%|‚ñà‚ñà‚ñà‚ñà      | 5.71G/14.1G [00:18<00:31, 267MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  41%|‚ñà‚ñà‚ñà‚ñà      | 5.75G/14.1G [00:18<00:35, 240MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  41%|‚ñà‚ñà‚ñà‚ñà      | 5.78G/14.1G [00:18<00:38, 216MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  41%|‚ñà‚ñà‚ñà‚ñà      | 5.81G/14.1G [00:18<00:37, 220MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5.84G/14.1G [00:18<00:38, 215MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5.87G/14.1G [00:19<00:40, 206MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5.89G/14.1G [00:19<00:41, 200MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5.92G/14.1G [00:19<00:37, 218MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5.96G/14.1G [00:19<00:34, 235MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5.99G/14.1G [00:19<00:32, 247MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6.02G/14.1G [00:19<00:31, 255MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6.05G/14.1G [00:19<00:31, 258MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6.08G/14.1G [00:19<00:30, 263MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6.11G/14.1G [00:19<00:30, 265MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6.14G/14.1G [00:20<00:29, 268MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6.18G/14.1G [00:20<00:37, 215MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 6.21G/14.1G [00:20<00:34, 229MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 6.24G/14.1G [00:20<00:38, 203MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 6.27G/14.1G [00:20<00:43, 181MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 6.29G/14.1G [00:21<00:47, 167MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 6.31G/14.1G [00:21<01:00, 129MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 6.33G/14.1G [00:21<01:07, 116MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 6.35G/14.1G [00:21<01:10, 110MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6.38G/14.1G [00:21<01:13, 106MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6.40G/14.1G [00:22<01:17, 99.6MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6.41G/14.1G [00:22<01:17, 99.7MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6.42G/14.1G [00:22<01:17, 99.1MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6.44G/14.1G [00:22<01:10, 109MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6.46G/14.1G [00:22<01:12, 106MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6.48G/14.1G [00:22<01:13, 104MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6.50G/14.1G [00:23<01:05, 117MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 6.52G/14.1G [00:23<01:00, 126MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6.54G/14.1G [00:23<01:05, 116MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6.56G/14.1G [00:23<01:00, 126MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6.59G/14.1G [00:23<00:56, 133MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6.61G/14.1G [00:23<00:55, 137MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6.63G/14.1G [00:24<00:57, 130MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6.65G/14.1G [00:24<00:56, 132MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6.67G/14.1G [00:24<00:55, 135MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6.69G/14.1G [00:24<00:50, 148MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 6.71G/14.1G [00:24<00:49, 152MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6.73G/14.1G [00:24<00:46, 158MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6.75G/14.1G [00:24<00:45, 161MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6.77G/14.1G [00:24<00:46, 160MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6.79G/14.1G [00:25<00:45, 160MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6.82G/14.1G [00:25<00:44, 165MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6.84G/14.1G [00:25<00:45, 162MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6.86G/14.1G [00:25<00:44, 165MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 6.88G/14.1G [00:25<00:45, 161MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 6.90G/14.1G [00:25<01:00, 120MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 6.95G/14.1G [00:26<00:40, 179MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 6.97G/14.1G [00:26<00:40, 175MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 6.99G/14.1G [00:26<00:41, 171MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 7.01G/14.1G [00:26<00:41, 173MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 7.04G/14.1G [00:26<00:41, 171MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 7.06G/14.1G [00:26<00:41, 172MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7.08G/14.1G [00:26<00:41, 170MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7.10G/14.1G [00:26<00:40, 172MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7.12G/14.1G [00:27<00:41, 169MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7.14G/14.1G [00:27<00:42, 164MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7.16G/14.1G [00:27<00:41, 170MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7.18G/14.1G [00:27<00:42, 165MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7.20G/14.1G [00:27<00:40, 170MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7.22G/14.1G [00:27<00:40, 169MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7.25G/14.1G [00:27<00:40, 172MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 7.27G/14.1G [00:27<00:41, 166MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 7.29G/14.1G [00:28<00:40, 170MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 7.31G/14.1G [00:28<00:40, 169MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 7.33G/14.1G [00:28<00:41, 165MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 7.35G/14.1G [00:28<00:40, 168MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 7.37G/14.1G [00:28<00:40, 166MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 7.39G/14.1G [00:28<00:40, 168MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 7.41G/14.1G [00:28<00:41, 161MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 7.43G/14.1G [00:28<00:40, 165MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 7.46G/14.1G [00:29<00:39, 168MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 7.48G/14.1G [00:29<00:39, 167MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 7.50G/14.1G [00:29<00:42, 156MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 7.52G/14.1G [00:29<00:40, 162MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 7.54G/14.1G [00:29<00:39, 167MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 7.56G/14.1G [00:29<00:39, 167MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 7.58G/14.1G [00:29<00:44, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7.60G/14.1G [00:30<00:41, 158MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7.62G/14.1G [00:30<00:40, 162MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7.64G/14.1G [00:30<00:40, 160MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7.67G/14.1G [00:30<00:43, 148MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7.69G/14.1G [00:30<00:40, 158MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7.71G/14.1G [00:30<00:40, 160MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7.73G/14.1G [00:30<00:40, 158MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7.75G/14.1G [00:30<00:43, 147MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 7.77G/14.1G [00:31<00:41, 154MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 7.79G/14.1G [00:31<00:38, 163MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 7.81G/14.1G [00:31<00:40, 157MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 7.83G/14.1G [00:31<00:41, 150MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 7.85G/14.1G [00:31<00:40, 155MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 7.87G/14.1G [00:31<00:38, 161MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 7.90G/14.1G [00:31<00:41, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 7.92G/14.1G [00:32<00:42, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 7.94G/14.1G [00:32<00:40, 155MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 7.96G/14.1G [00:32<00:39, 155MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 7.98G/14.1G [00:32<00:41, 150MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8.00G/14.1G [00:32<00:40, 152MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8.02G/14.1G [00:32<00:38, 158MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8.04G/14.1G [00:32<00:39, 156MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8.06G/14.1G [00:33<00:43, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8.08G/14.1G [00:33<00:40, 150MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8.11G/14.1G [00:33<00:38, 158MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8.13G/14.1G [00:33<00:41, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8.15G/14.1G [00:33<00:41, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8.17G/14.1G [00:33<00:39, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8.19G/14.1G [00:33<00:37, 159MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8.21G/14.1G [00:34<00:40, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8.23G/14.1G [00:34<00:39, 148MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8.25G/14.1G [00:34<00:36, 160MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8.27G/14.1G [00:34<00:39, 148MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 8.29G/14.1G [00:34<00:41, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8.32G/14.1G [00:34<00:40, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8.34G/14.1G [00:34<00:37, 154MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8.36G/14.1G [00:35<00:40, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8.38G/14.1G [00:35<00:41, 138MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8.40G/14.1G [00:35<00:38, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8.42G/14.1G [00:35<00:38, 148MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8.44G/14.1G [00:35<00:40, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 8.46G/14.1G [00:35<00:40, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 8.48G/14.1G [00:35<00:37, 151MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 8.50G/14.1G [00:36<00:49, 114MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 8.55G/14.1G [00:36<00:35, 156MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 8.57G/14.1G [00:36<00:34, 160MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 8.59G/14.1G [00:36<00:37, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 8.61G/14.1G [00:36<00:39, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 8.63G/14.1G [00:36<00:35, 153MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 8.65G/14.1G [00:37<00:36, 152MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8.67G/14.1G [00:37<00:38, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8.69G/14.1G [00:37<00:38, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8.71G/14.1G [00:37<00:37, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8.73G/14.1G [00:37<00:38, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8.76G/14.1G [00:37<00:37, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8.78G/14.1G [00:37<00:37, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8.80G/14.1G [00:38<00:36, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 8.82G/14.1G [00:38<00:38, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 8.84G/14.1G [00:38<00:38, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 8.86G/14.1G [00:38<00:35, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 8.88G/14.1G [00:38<00:37, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 8.90G/14.1G [00:38<00:38, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 8.92G/14.1G [00:38<00:36, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 8.94G/14.1G [00:39<00:35, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 8.97G/14.1G [00:39<00:37, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 8.99G/14.1G [00:39<00:36, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 9.01G/14.1G [00:39<00:34, 150MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9.03G/14.1G [00:39<00:36, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9.05G/14.1G [00:39<00:36, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9.07G/14.1G [00:39<00:35, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9.09G/14.1G [00:40<00:34, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9.11G/14.1G [00:40<00:36, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9.13G/14.1G [00:40<00:35, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9.15G/14.1G [00:40<00:35, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9.18G/14.1G [00:40<00:36, 137MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 9.20G/14.1G [00:40<00:35, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 9.22G/14.1G [00:41<00:34, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 9.24G/14.1G [00:41<00:35, 138MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 9.26G/14.1G [00:41<00:34, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 9.28G/14.1G [00:41<00:34, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 9.30G/14.1G [00:41<00:35, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 9.32G/14.1G [00:41<00:34, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 9.34G/14.1G [00:41<00:33, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 9.36G/14.1G [00:42<00:33, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 9.38G/14.1G [00:42<00:33, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 9.41G/14.1G [00:42<00:33, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 9.43G/14.1G [00:42<00:33, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 9.45G/14.1G [00:42<00:32, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 9.47G/14.1G [00:42<00:32, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 9.49G/14.1G [00:42<00:33, 138MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 9.51G/14.1G [00:43<00:32, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 9.53G/14.1G [00:43<00:32, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9.55G/14.1G [00:43<00:32, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9.57G/14.1G [00:43<00:31, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9.59G/14.1G [00:43<00:31, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9.62G/14.1G [00:43<00:32, 138MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9.64G/14.1G [00:44<00:32, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9.66G/14.1G [00:44<00:31, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9.68G/14.1G [00:44<00:31, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 9.70G/14.1G [00:44<00:30, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9.72G/14.1G [00:44<00:30, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9.74G/14.1G [00:44<00:30, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9.76G/14.1G [00:44<00:30, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9.78G/14.1G [00:45<00:30, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9.80G/14.1G [00:45<00:29, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9.83G/14.1G [00:45<00:29, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9.85G/14.1G [00:45<00:29, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9.87G/14.1G [00:45<00:30, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 9.89G/14.1G [00:45<00:29, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 9.91G/14.1G [00:45<00:30, 138MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 9.93G/14.1G [00:46<00:28, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 9.95G/14.1G [00:46<00:40, 104MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 9.99G/14.1G [00:46<00:26, 159MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 10.0G/14.1G [00:46<00:26, 155MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 10.0G/14.1G [00:46<00:27, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 10.1G/14.1G [00:46<00:28, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10.1G/14.1G [00:47<00:29, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10.1G/14.1G [00:47<00:31, 129MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10.1G/14.1G [00:47<00:27, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10.1G/14.1G [00:47<00:26, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10.2G/14.1G [00:47<00:26, 148MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10.2G/14.1G [00:47<00:26, 148MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10.2G/14.1G [00:47<00:28, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10.2G/14.1G [00:48<00:27, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10.2G/14.1G [00:48<00:27, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 10.3G/14.1G [00:48<00:26, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 10.3G/14.1G [00:48<00:26, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 10.3G/14.1G [00:48<00:26, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 10.3G/14.1G [00:48<00:26, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 10.3G/14.1G [00:48<00:26, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 10.4G/14.1G [00:49<00:25, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 10.4G/14.1G [00:49<00:25, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 10.4G/14.1G [00:49<00:25, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10.4G/14.1G [00:49<00:25, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10.5G/14.1G [00:49<00:25, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10.5G/14.1G [00:49<00:25, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10.5G/14.1G [00:50<00:26, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10.5G/14.1G [00:50<00:29, 123MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10.5G/14.1G [00:50<00:30, 119MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10.6G/14.1G [00:50<00:31, 113MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10.6G/14.1G [00:50<00:31, 113MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 10.6G/14.1G [00:51<00:31, 111MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 10.6G/14.1G [00:51<00:33, 104MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 10.6G/14.1G [00:51<00:31, 112MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 10.7G/14.1G [00:51<00:31, 110MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 10.7G/14.1G [00:51<00:29, 118MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 10.7G/14.1G [00:51<00:29, 115MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 10.7G/14.1G [00:52<00:29, 117MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 10.7G/14.1G [00:52<00:27, 123MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 10.8G/14.1G [00:52<00:26, 128MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10.8G/14.1G [00:52<00:26, 127MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10.8G/14.1G [00:52<00:26, 126MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10.8G/14.1G [00:52<00:26, 126MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10.9G/14.1G [00:53<00:24, 131MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10.9G/14.1G [00:53<00:25, 129MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10.9G/14.1G [00:53<00:24, 134MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10.9G/14.1G [00:53<00:23, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 10.9G/14.1G [00:53<00:22, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11.0G/14.1G [00:53<00:22, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11.0G/14.1G [00:53<00:22, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11.0G/14.1G [00:54<00:21, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11.0G/14.1G [00:54<00:21, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11.0G/14.1G [00:54<00:20, 154MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11.1G/14.1G [00:54<00:17, 172MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11.1G/14.1G [00:54<00:16, 186MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 11.1G/14.1G [00:54<00:15, 196MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 11.2G/14.1G [00:54<00:14, 199MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 11.2G/14.1G [00:55<00:14, 203MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 11.2G/14.1G [00:55<00:15, 188MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 11.3G/14.1G [00:55<00:15, 188MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 11.3G/14.1G [00:55<00:14, 194MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 11.3G/14.1G [00:55<00:14, 192MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 11.3G/14.1G [00:55<00:16, 169MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 11.3G/14.1G [00:55<00:15, 177MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 11.4G/14.1G [00:56<00:15, 181MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 11.4G/14.1G [00:56<00:17, 154MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 11.4G/14.1G [00:56<00:17, 155MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 11.4G/14.1G [00:56<00:22, 120MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 11.5G/14.1G [00:56<00:18, 147MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 11.5G/14.1G [00:56<00:16, 157MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 11.5G/14.1G [00:57<00:15, 174MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 11.5G/14.1G [00:57<00:14, 177MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 11.6G/14.1G [00:57<00:16, 153MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 11.6G/14.1G [00:57<00:15, 163MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 11.6G/14.1G [00:57<00:14, 180MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 11.6G/14.1G [00:57<00:15, 158MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 11.6G/14.1G [00:57<00:15, 160MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 11.7G/14.1G [00:58<00:13, 178MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 11.7G/14.1G [00:58<00:15, 162MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 11.7G/14.1G [00:58<00:15, 153MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 11.8G/14.1G [00:58<00:13, 172MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 11.8G/14.1G [00:58<00:14, 162MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 11.8G/14.1G [00:58<00:15, 147MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 11.8G/14.1G [00:58<00:14, 159MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 11.8G/14.1G [00:59<00:13, 169MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11.9G/14.1G [00:59<00:15, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11.9G/14.1G [00:59<00:15, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11.9G/14.1G [00:59<00:14, 160MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11.9G/14.1G [00:59<00:14, 153MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 11.9G/14.1G [00:59<00:15, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 12.0G/14.1G [00:59<00:14, 153MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 12.0G/14.1G [01:00<00:13, 158MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 12.0G/14.1G [01:00<00:15, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12.0G/14.1G [01:00<00:13, 154MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12.1G/14.1G [01:00<00:13, 159MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12.1G/14.1G [01:00<00:14, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12.1G/14.1G [01:00<00:13, 151MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12.1G/14.1G [01:00<00:12, 163MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12.2G/14.1G [01:01<00:13, 148MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12.2G/14.1G [01:01<00:13, 141MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 12.2G/14.1G [01:01<00:11, 162MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 12.2G/14.1G [01:01<00:12, 147MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 12.2G/14.1G [01:01<00:13, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 12.3G/14.1G [01:01<00:11, 161MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 12.3G/14.1G [01:02<00:12, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 12.3G/14.1G [01:02<00:13, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 12.3G/14.1G [01:02<00:12, 139MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 12.4G/14.1G [01:02<00:11, 151MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 12.4G/14.1G [01:02<00:12, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 12.4G/14.1G [01:02<00:11, 147MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 12.4G/14.1G [01:03<00:15, 114MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 12.5G/14.1G [01:03<00:11, 147MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 12.5G/14.1G [01:03<00:10, 159MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 12.5G/14.1G [01:03<00:10, 151MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 12.5G/14.1G [01:03<00:11, 137MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 12.6G/14.1G [01:03<00:09, 162MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 12.6G/14.1G [01:04<00:10, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 12.6G/14.1G [01:04<00:11, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 12.6G/14.1G [01:04<00:10, 149MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 12.6G/14.1G [01:04<00:10, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 12.7G/14.1G [01:04<00:11, 133MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 12.7G/14.1G [01:04<00:10, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 12.7G/14.1G [01:05<00:09, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12.7G/14.1G [01:05<00:10, 132MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12.8G/14.1G [01:05<00:10, 134MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12.8G/14.1G [01:05<00:09, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12.8G/14.1G [01:05<00:10, 132MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12.8G/14.1G [01:05<00:09, 134MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12.8G/14.1G [01:05<00:08, 147MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12.9G/14.1G [01:06<00:08, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 12.9G/14.1G [01:06<00:09, 130MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12.9G/14.1G [01:06<00:08, 151MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 12.9G/14.1G [01:06<00:12, 101MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 13.0G/14.1G [01:06<00:09, 126MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 13.0G/14.1G [01:07<00:07, 155MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 13.0G/14.1G [01:07<00:06, 163MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 13.0G/14.1G [01:07<00:06, 162MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 13.1G/14.1G [01:07<00:06, 158MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 13.1G/14.1G [01:07<00:07, 137MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13.1G/14.1G [01:07<00:07, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13.1G/14.1G [01:07<00:06, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13.1G/14.1G [01:08<00:07, 131MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13.2G/14.1G [01:08<00:07, 128MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13.2G/14.1G [01:08<00:06, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13.2G/14.1G [01:08<00:06, 135MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13.2G/14.1G [01:08<00:07, 128MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13.2G/14.1G [01:08<00:06, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 13.3G/14.1G [01:09<00:06, 129MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 13.3G/14.1G [01:09<00:07, 119MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 13.3G/14.1G [01:09<00:05, 147MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 13.3G/14.1G [01:09<00:05, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 13.4G/14.1G [01:09<00:05, 132MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 13.4G/14.1G [01:09<00:05, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 13.4G/14.1G [01:10<00:05, 132MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13.4G/14.1G [01:10<00:05, 133MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13.5G/14.1G [01:10<00:04, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13.5G/14.1G [01:10<00:05, 131MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13.5G/14.1G [01:10<00:05, 126MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13.5G/14.1G [01:10<00:04, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13.5G/14.1G [01:11<00:04, 130MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13.6G/14.1G [01:11<00:04, 136MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 13.6G/14.1G [01:11<00:03, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 13.6G/14.1G [01:11<00:04, 131MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 13.6G/14.1G [01:11<00:03, 129MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 13.7G/14.1G [01:11<00:03, 145MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 13.7G/14.1G [01:12<00:03, 138MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 13.7G/14.1G [01:12<00:03, 125MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 13.7G/14.1G [01:12<00:02, 146MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 13.7G/14.1G [01:12<00:02, 134MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 13.8G/14.1G [01:12<00:02, 132MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 13.8G/14.1G [01:12<00:02, 143MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 13.8G/14.1G [01:13<00:02, 130MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 13.8G/14.1G [01:13<00:02, 126MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 13.9G/14.1G [01:13<00:01, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 13.9G/14.1G [01:13<00:01, 131MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 13.9G/14.1G [01:13<00:01, 131MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 13.9G/14.1G [01:14<00:01, 140MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 14.0G/14.1G [01:14<00:01, 129MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 14.0G/14.1G [01:14<00:01, 137MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 14.0G/14.1G [01:14<00:00, 142MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 14.0G/14.1G [01:14<00:00, 131MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 14.0G/14.1G [01:14<00:00, 135MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 14.1G/14.1G [01:14<00:00, 144MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 14.1G/14.1G [01:15<00:00, 132MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 14.1G/14.1G [01:15<00:00, 130MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 14.1G/14.1G [01:15<00:00, 125MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1G/14.1G [01:15<00:00, 127MB/s]\n",
      "Downloading (‚Ä¶)\"pytorch_model.bin\";: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1G/14.1G [01:15<00:00, 187MB/s]\n",
      "trainable params: 3932160 || all params: 7072948224 || trainable%: 0.055594355783029126\n",
      "0%|          | 0/3867 [00:00<?, ?it/s]\n",
      "[2023-04-06 14:29:54.391: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.27.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "[2023-04-06 14:29:54.431 algo-1:87 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-04-06 14:29:54.466 algo-1:87 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2023-04-06 14:29:54.466 algo-1:87 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-04-06 14:29:54.467 algo-1:87 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-04-06 14:29:54.467 algo-1:87 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-04-06 14:29:54.467 algo-1:87 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "0%|          | 1/3867 [00:07<8:04:30,  7.52s/it]\n",
      "0%|          | 2/3867 [00:12<6:21:45,  5.93s/it]\n",
      "0%|          | 3/3867 [00:17<5:48:44,  5.42s/it]\n",
      "0%|          | 4/3867 [00:21<5:33:59,  5.19s/it]\n",
      "0%|          | 5/3867 [00:26<5:25:01,  5.05s/it]\n",
      "0%|          | 6/3867 [00:31<5:19:24,  4.96s/it]\n",
      "0%|          | 7/3867 [00:36<5:15:57,  4.91s/it]\n",
      "0%|          | 8/3867 [00:41<5:13:35,  4.88s/it]\n",
      "0%|          | 9/3867 [00:45<5:12:07,  4.85s/it]\n",
      "0%|          | 10/3867 [00:50<5:11:04,  4.84s/it]\n",
      "0%|          | 10/3867 [00:50<5:11:04,  4.84s/it]\n",
      "{'loss': 2.7628, 'learning_rate': 0.00019948280320662015, 'epoch': 0.01}\n",
      "0%|          | 11/3867 [00:55<5:10:20,  4.83s/it]\n",
      "0%|          | 12/3867 [01:00<5:09:44,  4.82s/it]\n",
      "0%|          | 13/3867 [01:05<5:09:20,  4.82s/it]\n",
      "0%|          | 14/3867 [01:10<5:09:43,  4.82s/it]\n",
      "0%|          | 15/3867 [01:14<5:09:21,  4.82s/it]\n",
      "0%|          | 16/3867 [01:19<5:09:09,  4.82s/it]\n",
      "0%|          | 17/3867 [01:24<5:08:51,  4.81s/it]\n",
      "0%|          | 18/3867 [01:29<5:08:45,  4.81s/it]\n",
      "0%|          | 19/3867 [01:34<5:08:41,  4.81s/it]\n",
      "1%|          | 20/3867 [01:38<5:08:29,  4.81s/it]\n",
      "1%|          | 20/3867 [01:38<5:08:29,  4.81s/it]\n",
      "{'loss': 2.447, 'learning_rate': 0.00019896560641324024, 'epoch': 0.02}\n",
      "1%|          | 21/3867 [01:43<5:08:20,  4.81s/it]\n",
      "1%|          | 22/3867 [01:48<5:08:40,  4.82s/it]\n",
      "1%|          | 23/3867 [01:53<5:08:38,  4.82s/it]\n",
      "1%|          | 24/3867 [01:58<5:08:31,  4.82s/it]\n",
      "1%|          | 25/3867 [02:02<5:08:22,  4.82s/it]\n",
      "1%|          | 26/3867 [02:07<5:08:09,  4.81s/it]\n",
      "1%|          | 27/3867 [02:12<5:08:08,  4.81s/it]\n",
      "1%|          | 28/3867 [02:17<5:08:03,  4.81s/it]\n",
      "1%|          | 29/3867 [02:22<5:07:56,  4.81s/it]\n",
      "1%|          | 30/3867 [02:27<5:07:46,  4.81s/it]\n",
      "{'loss': 2.4206, 'learning_rate': 0.00019844840961986038, 'epoch': 0.02}\n",
      "1%|          | 30/3867 [02:27<5:07:46,  4.81s/it]\n",
      "1%|          | 31/3867 [02:31<5:08:17,  4.82s/it]\n",
      "1%|          | 32/3867 [02:36<5:08:29,  4.83s/it]\n",
      "1%|          | 33/3867 [02:41<5:08:06,  4.82s/it]\n",
      "1%|          | 34/3867 [02:46<5:07:46,  4.82s/it]\n",
      "1%|          | 35/3867 [02:51<5:07:32,  4.82s/it]\n",
      "1%|          | 36/3867 [02:55<5:07:22,  4.81s/it]\n",
      "1%|          | 37/3867 [03:00<5:07:13,  4.81s/it]\n",
      "1%|          | 38/3867 [03:05<5:07:06,  4.81s/it]\n",
      "1%|          | 39/3867 [03:10<5:07:39,  4.82s/it]\n",
      "1%|          | 40/3867 [03:15<5:07:25,  4.82s/it]\n",
      "{'loss': 2.3144, 'learning_rate': 0.0001979312128264805, 'epoch': 0.03}\n",
      "1%|          | 40/3867 [03:15<5:07:25,  4.82s/it]\n",
      "1%|          | 41/3867 [03:20<5:07:10,  4.82s/it]\n",
      "1%|          | 42/3867 [03:24<5:06:59,  4.82s/it]\n",
      "1%|          | 43/3867 [03:29<5:06:52,  4.81s/it]\n",
      "1%|          | 44/3867 [03:34<5:06:46,  4.81s/it]\n",
      "1%|          | 45/3867 [03:39<5:06:47,  4.82s/it]\n",
      "1%|          | 46/3867 [03:44<5:06:40,  4.82s/it]\n",
      "1%|          | 47/3867 [03:48<5:06:32,  4.81s/it]\n",
      "1%|          | 48/3867 [03:53<5:06:26,  4.81s/it]\n",
      "1%|‚ñè         | 49/3867 [03:58<5:07:27,  4.83s/it]\n",
      "1%|‚ñè         | 50/3867 [04:03<5:07:14,  4.83s/it]\n",
      "{'loss': 2.3493, 'learning_rate': 0.0001974140160331006, 'epoch': 0.04}\n",
      "1%|‚ñè         | 50/3867 [04:03<5:07:14,  4.83s/it]\n",
      "1%|‚ñè         | 51/3867 [04:08<5:07:03,  4.83s/it]\n",
      "1%|‚ñè         | 52/3867 [04:13<5:06:42,  4.82s/it]\n",
      "1%|‚ñè         | 53/3867 [04:17<5:06:25,  4.82s/it]\n",
      "1%|‚ñè         | 54/3867 [04:22<5:06:09,  4.82s/it]\n",
      "1%|‚ñè         | 55/3867 [04:27<5:06:01,  4.82s/it]\n",
      "1%|‚ñè         | 56/3867 [04:32<5:05:57,  4.82s/it]\n",
      "1%|‚ñè         | 57/3867 [04:37<5:05:50,  4.82s/it]\n",
      "1%|‚ñè         | 58/3867 [04:42<5:06:19,  4.83s/it]\n",
      "2%|‚ñè         | 59/3867 [04:46<5:06:04,  4.82s/it]\n",
      "2%|‚ñè         | 60/3867 [04:51<5:05:43,  4.82s/it]\n",
      "{'loss': 2.3659, 'learning_rate': 0.00019689681923972072, 'epoch': 0.05}\n",
      "2%|‚ñè         | 60/3867 [04:51<5:05:43,  4.82s/it]\n",
      "2%|‚ñè         | 61/3867 [04:56<5:05:31,  4.82s/it]\n",
      "2%|‚ñè         | 62/3867 [05:01<5:05:26,  4.82s/it]\n",
      "2%|‚ñè         | 63/3867 [05:06<5:05:18,  4.82s/it]\n",
      "2%|‚ñè         | 64/3867 [05:10<5:05:22,  4.82s/it]\n",
      "2%|‚ñè         | 65/3867 [05:15<5:05:26,  4.82s/it]\n",
      "2%|‚ñè         | 66/3867 [05:20<5:05:16,  4.82s/it]\n",
      "2%|‚ñè         | 67/3867 [05:25<5:05:16,  4.82s/it]\n",
      "2%|‚ñè         | 68/3867 [05:30<5:05:08,  4.82s/it]\n",
      "2%|‚ñè         | 69/3867 [05:35<5:05:04,  4.82s/it]\n",
      "2%|‚ñè         | 70/3867 [05:39<5:04:53,  4.82s/it]\n",
      "{'loss': 2.4107, 'learning_rate': 0.00019637962244634086, 'epoch': 0.05}\n",
      "2%|‚ñè         | 70/3867 [05:39<5:04:53,  4.82s/it]\n",
      "2%|‚ñè         | 71/3867 [05:44<5:04:42,  4.82s/it]\n",
      "2%|‚ñè         | 72/3867 [05:49<5:04:29,  4.81s/it]\n",
      "2%|‚ñè         | 73/3867 [05:54<5:04:36,  4.82s/it]\n",
      "2%|‚ñè         | 74/3867 [05:59<5:04:31,  4.82s/it]\n",
      "2%|‚ñè         | 75/3867 [06:03<5:05:15,  4.83s/it]\n",
      "2%|‚ñè         | 76/3867 [06:08<5:05:06,  4.83s/it]\n",
      "2%|‚ñè         | 77/3867 [06:13<5:04:51,  4.83s/it]\n",
      "2%|‚ñè         | 78/3867 [06:18<5:04:33,  4.82s/it]\n",
      "2%|‚ñè         | 79/3867 [06:23<5:04:21,  4.82s/it]\n",
      "2%|‚ñè         | 80/3867 [06:28<5:04:15,  4.82s/it]\n",
      "{'loss': 2.3378, 'learning_rate': 0.00019586242565296095, 'epoch': 0.06}\n",
      "2%|‚ñè         | 80/3867 [06:28<5:04:15,  4.82s/it]\n",
      "2%|‚ñè         | 81/3867 [06:32<5:04:02,  4.82s/it]\n",
      "2%|‚ñè         | 82/3867 [06:37<5:03:59,  4.82s/it]\n",
      "2%|‚ñè         | 83/3867 [06:42<5:03:52,  4.82s/it]\n",
      "2%|‚ñè         | 84/3867 [06:47<5:04:15,  4.83s/it]\n",
      "2%|‚ñè         | 85/3867 [06:52<5:03:54,  4.82s/it]\n",
      "2%|‚ñè         | 86/3867 [06:56<5:03:49,  4.82s/it]\n",
      "2%|‚ñè         | 87/3867 [07:01<5:03:39,  4.82s/it]\n",
      "2%|‚ñè         | 88/3867 [07:06<5:03:32,  4.82s/it]\n",
      "2%|‚ñè         | 89/3867 [07:11<5:03:22,  4.82s/it]\n",
      "2%|‚ñè         | 90/3867 [07:16<5:03:18,  4.82s/it]\n",
      "{'loss': 2.3588, 'learning_rate': 0.0001953452288595811, 'epoch': 0.07}\n",
      "2%|‚ñè         | 90/3867 [07:16<5:03:18,  4.82s/it]\n",
      "2%|‚ñè         | 91/3867 [07:21<5:03:15,  4.82s/it]\n",
      "2%|‚ñè         | 92/3867 [07:25<5:03:02,  4.82s/it]\n",
      "2%|‚ñè         | 93/3867 [07:30<5:03:33,  4.83s/it]\n",
      "2%|‚ñè         | 94/3867 [07:35<5:03:18,  4.82s/it]\n",
      "2%|‚ñè         | 95/3867 [07:40<5:03:03,  4.82s/it]\n",
      "2%|‚ñè         | 96/3867 [07:45<5:02:50,  4.82s/it]\n",
      "3%|‚ñé         | 97/3867 [07:49<5:02:49,  4.82s/it]\n",
      "3%|‚ñé         | 98/3867 [07:54<5:02:39,  4.82s/it]\n",
      "3%|‚ñé         | 99/3867 [07:59<5:02:37,  4.82s/it]\n",
      "3%|‚ñé         | 100/3867 [08:04<5:02:32,  4.82s/it]\n",
      "{'loss': 2.3371, 'learning_rate': 0.0001948280320662012, 'epoch': 0.08}\n",
      "3%|‚ñé         | 100/3867 [08:04<5:02:32,  4.82s/it]\n",
      "3%|‚ñé         | 101/3867 [08:09<5:02:29,  4.82s/it]\n",
      "3%|‚ñé         | 102/3867 [08:14<5:04:23,  4.85s/it]\n",
      "3%|‚ñé         | 103/3867 [08:19<5:03:39,  4.84s/it]\n",
      "3%|‚ñé         | 104/3867 [08:23<5:03:09,  4.83s/it]\n",
      "3%|‚ñé         | 105/3867 [08:28<5:02:44,  4.83s/it]\n",
      "3%|‚ñé         | 106/3867 [08:33<5:02:25,  4.82s/it]\n",
      "3%|‚ñé         | 107/3867 [08:38<5:02:04,  4.82s/it]\n",
      "3%|‚ñé         | 108/3867 [08:43<5:01:51,  4.82s/it]\n",
      "3%|‚ñé         | 109/3867 [08:47<5:01:41,  4.82s/it]\n",
      "3%|‚ñé         | 110/3867 [08:52<5:01:57,  4.82s/it]\n",
      "{'loss': 2.3149, 'learning_rate': 0.00019431083527282132, 'epoch': 0.09}\n",
      "3%|‚ñé         | 110/3867 [08:52<5:01:57,  4.82s/it]\n",
      "3%|‚ñé         | 111/3867 [08:57<5:01:41,  4.82s/it]\n",
      "3%|‚ñé         | 112/3867 [09:02<5:01:34,  4.82s/it]\n",
      "3%|‚ñé         | 113/3867 [09:07<5:01:29,  4.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `peft` for training, you normally end up with adapter weights. We added the `merge_and_unload()` method to merge the base model with the adatper to make it easier to deploy the model. Since we can now use the `pipelines` feature of the `transformers` library. \n",
    "\n",
    "We can now deploy our model using the `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the model to create a predictor\n",
    "predictor = huggingface_estimator.deploy(1, \"ml.g5.4xlarge\")\n",
    "\n",
    "# if you started the training and closed the notebook you can use the code below to deploy it \n",
    "# from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# # create Hugging Face Model Class\n",
    "# huggingface_model = HuggingFaceModel(\n",
    "#    model_data=\"s3://hf-sagemaker-inference/model.tar.gz\",  # Change to your model path\n",
    "#    role=role, \n",
    "#    transformers_version=\"4.26\", \n",
    "#    pytorch_version=\"1.13\", \n",
    "#    py_version=\"py39\",\n",
    "# )\n",
    "# # deploy model to SageMaker Inference\n",
    "# predictor = huggingface_model.deploy(\n",
    "#    initial_instance_count=1,\n",
    "#    instance_type= \"ml.g5.4xlarge\"\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker starts the deployment process by creating a SageMaker Endpoint Configuration and a SageMaker Endpoint. The Endpoint Configuration defines the model and the instance type.\n",
    "\n",
    "Lets test by using a example from the `test` split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")\n",
    "\n",
    "# select a random test sample\n",
    "sample = test_dataset[randint(0,len(test_dataset))]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "fomatted_sample = {\n",
    "  \"inputs\": prompt_template.format(dialogue=sample[\"dialogue\"]),\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 50,\n",
    "    \"temperature\": 0.8\n",
    "  }\n",
    "}\n",
    "\n",
    "# predict\n",
    "res = predictor.predict(fomatted_sample)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare it to the test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[\"summary\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
