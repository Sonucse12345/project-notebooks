{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate Mixtral 8x7B with Speculative Decoding and Quantziation on Amazon SageMaker\n",
    "\n",
    "\n",
    "In this blog you will learn how to accelerate [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) on Amazon SageMaker using Speculative Decoding (Medusa) and Quantization (AWQ). We are going to use the Hugging Face LLM DLC a purpose-built Inference Container powered by [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) a scalelable, optimized solution for deploying and serving Large Language Models (LLMs). We will prepare the artifacts locally, upload them to Amazon S3, and deploy the model to Amazon SageMaker. \n",
    "\n",
    "Combining Medusa and AWQ allows not only to reduce the memory footprint but also to accelerate the inference of Mixtral 8x7B. With this we can deploy Mixtral 8x7B on a single `g5.12xlarge` instance with only 4x NVIDIA A10G GPUs achieving a latency of ~50-60ms per token. \n",
    "\n",
    "[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) is an open LLM from [Mistral AI](https://huggingface.co/mistralai) Mixtral-8x7B is a Sparse Mixture of Experts. If you want to learn more about MoEs check out [Mixture of Experts Explained](https://huggingface.co/blog/moe). In the blog will cover how to:\n",
    "\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Retrieve the new Hugging Face LLM DLC](#2-retrieve-the-new-hugging-face-llm-dlc)\n",
    "3. [Prepare Medusa and AWQ artifacts](#3-prepare-medusa-and-awq-artifacts)\n",
    "4. [Deploy Mixtral 8x7B to Amazon SageMaker](#4-deploy-mixtral-8x7b-to-amazon-sagemaker)\n",
    "5. [Run inference and chat with the model](#5-run-inference-and-chat-with-the-model)\n",
    "6. [Clean up](#5-clean-up)\n",
    "\n",
    "Lets get started!\n",
    "\n",
    "Before we go into the details, let's first understand what Speculative Decoding and Quantization are and how they can help accelerate Mixtral 8x7B.\n",
    "\n",
    "**Speculative Decoding (Medusa)**\n",
    "\n",
    "[Medusa](https://github.com/FasterDecoding/Medusa/tree/main) is a speculative decoding like technique that can be used to accelerate inference for large language models. It adds extra \"heads\" to LLMs to predict multiple future tokens simultaneously. To use Medusa, you need to train medusa heads on a small dataset preferably similar to the original training data. Medusa can deliver up to a 2x speed increase across a range of LLMs. We are going to use [text-generation-inference/Mixtral-8x7B-Instruct-v0.1-medusa](https://huggingface.co/text-generation-inference/Mixtral-8x7B-Instruct-v0.1-medusa). The performance improvements depends on the input prompt and can vary. \n",
    "\n",
    "**Quantization (AWQ)**\n",
    "\n",
    "[AWQ or Activation-aware Weight Quantization](https://github.com/mit-han-lab/llm-awq) is a method designed for efficiently compressing and accelerating LLMs by quantizing weights based on the significance of their activations. Resulting in significant reduced memory requirements and improved model latency. AWQ can reduce the memory footprint of a model by up to 4x. We are going to use [ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ](https://huggingface.co/ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ) a awq quantized version of Mixtral 8x7B Instruct.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy Mixtral to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.199.0\" transformers --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve the new Hugging Face LLM DLC\n",
    "\n",
    "Compared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our `HuggingFaceModel` model class with a `image_uri` pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the `get_huggingface_llm_image_uri` method provided by the `sagemaker` SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified `backend`, `session`, `region`, and `version`. You can find the available versions [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Medusa and AWQ artifacts\n",
    "\n",
    "In this step we are going to prepare the Medusa and AWQ artifacts. Hugging Face Text Generation Inference container supports [Medusa](https://huggingface.co/docs/text-generation-inference/conceptual/speculation) and AWQ. For Medusa we need to provide the a [config.json](https://huggingface.co/text-generation-inference/Mixtral-8x7B-Instruct-v0.1-medusa/blob/main/config.json) with the medusa specifications, including `base_model_name_or_path` and `medusa_lm_head.safetensors`. If we only wanted to deploy the Medusa model we could use `text-generation-inference/Mixtral-8x7B-Instruct-v0.1-medusa` as `HF_MODEL_ID`. \n",
    "But since we want to combine Medusa and AWQ and deploy from Amazon S3 we need to modify the `config.json` to include the AWQ model. We are going to use the [ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ](https://huggingface.co/ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ) model. \n",
    "\n",
    "This will lead to a directory structure like this:\n",
    "\n",
    "```\n",
    "model/\n",
    "├── llm/\n",
    "│   ├── config.json\n",
    "│   ├── model-00001-of-00005.safetensors\n",
    "│   ├── ...\n",
    "├── config.json\n",
    "├── medusa_lm_head.safetensors\n",
    "└── ...\n",
    "```\n",
    "\n",
    "We will first load the Medusa model and AWQ model from Hugging Face, modify the `config.json` and then upload the artifacts to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "tmp_dir = \"./tmp\"\n",
    "medusa_repository = \"text-generation-inference/Mixtral-8x7B-Instruct-v0.1-medusa\" # Medusa model\n",
    "# https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ/discussions/6\n",
    "llm_repository = \"ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ\" # AWQ LLM model\n",
    "\n",
    "snapshot_download(repo_id=medusa_repository, local_dir=tmp_dir)\n",
    "snapshot_download(repo_id=llm_repository, local_dir=os.path.join(tmp_dir, \"llm\"),ignore_patterns=\"*.bin\")\n",
    "\n",
    "# rewrite meudsa base model value\n",
    "with open(os.path.join(tmp_dir, \"config.json\"), \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    data[\"base_model_name_or_path\"] = \"/opt/ml/model/llm/\" # path to llm model in side Amazon SageMaker\n",
    "with open(os.path.join(tmp_dir, \"config.json\"), \"w\") as f_out:\n",
    "    json.dump(data, indent=2, fp=f_out)\n",
    "    \n",
    "# upload the model to s3\n",
    "s3_path = S3Uploader.upload(\n",
    "    local_path=tmp_dir,\n",
    "    desired_s3_uri=f\"s3://{sess.default_bucket()}/medusa/mixtral\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy Mixtral 8x7B to Amazon SageMaker\n",
    "\n",
    "After we have prepared the Medusa and AWQ artifacts we can deploy Mixtral 8x7B to Amazon SageMaker. We will use the `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc. We will use a `g5.12xlarge` instance type, which has 4 NVIDIA A10G GPUs and 96GB of GPU memory.\n",
    "\n",
    "We make sure to have `QUANTIZE` set to `awq` and that `HF_MODEL_ID` is set to `/opt/ml/model`, since SageMaker will load the model from S3 to there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 300\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(8000),  # Max length of input text\n",
    "  'MAX_BATCH_PREFILL_TOKENS': json.dumps(16384),  # Number of tokens for the prefill operation.\n",
    "  'MAX_TOTAL_TOKENS': json.dumps(16384),  # Max length of the generation (including input text)\n",
    "  'QUANTIZE': \"awq\" # Quantization method\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  # path to s3 bucket with model, we are not using a compressed model \n",
    "  model_data={'S3DataSource':{'S3Uri': s3_path + \"/\",'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. TGI will automatically distribute and shard the model across all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run inference and chat with the model\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. We will use the `predict` method from the `predictor` to run inference on our endpoint. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [here](https://www.philschmid.de/sagemaker-llama-llm#5-run-inference-and-chat-with-the-model) or in the open api specification of the TGI in the [swagger documentation](https://huggingface.github.io/text-generation-inference/)\n",
    "\n",
    "The `mistralai/Mixtral-8x7B-Instruct-v0.1` is a conversational chat model meaning we can chat with it using the following prompt:\n",
    "  \n",
    "```\n",
    "<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2 [/INST]\n",
    "```\n",
    "Lets see, if Mixtral can come up with some cool ideas for the summer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer \n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ\")\n",
    "\n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    { \"role\": \"user\", \"content\": \"How can i make a good american cheese cake? Explain it step-by-step and include time estimates.\" },\n",
    "]\n",
    "\n",
    "# Generation arguments\n",
    "payload = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"return_full_text\": False,\n",
    "    \"stop\": [\"</s>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = llm.predict({\n",
    "  \"inputs\":tok.apply_chat_template(messages,tokenize=False,add_generation_prompt=True), # convert messages to model input\n",
    "  \"parameters\":payload\n",
    "})\n",
    "\n",
    "print(chat[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, We have successfully deployed Mixtral 8x7B to Amazon SageMaker with Medusa and AWQ on a `g5.12xlarge` instance. Thats not possible with the regular Mixtral 8x7B model. If we now look up the latency of the model on Cloudwatch we can see that we have a latency of ~50-60ms per token. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
