{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWtHKtNM3YI9"
      },
      "source": [
        "# Fine-tuning for Depth Estimation with ðŸ¤— Transformers\n",
        "\n",
        "This notebook shows how to fine-tune a pre-trained Vision model for Depth Estimation on a custom dataset. The idea is to add a randomly initialized depth estimator head on top of a pre-trained encoder and fine-tune the model altogether on a labeled dataset.\n",
        "\n",
        "\n",
        "## Dataset\n",
        "\n",
        "This notebook uses a subset of the [DIODE dataset](https://diode-dataset.org/). We'll be using a subset of the dataset to keep the runtime of the tutorial short.  \n",
        "\n",
        "## Model\n",
        "\n",
        "We'll fine-tune the [GLPN model](https://huggingface.co/docs/transformers/model_doc/glpn), which was pre-trained on the [NYU dataset](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html). You can find the other depth estimation networks on Hub with [this URL](https://huggingface.co/models?other=depth-estimation). \n",
        "\n",
        "**Note** that for models where there's no estimation head already available you'll have to manually attach it (randomly initialized). But this is not the case for GLPN since we already have a [`GLPNForDepthEstimation`](https://huggingface.co/docs/transformers/model_doc/glpn#transformers.GLPNForDepthEstimation) class.\n",
        "\n",
        "## Data preprocessing\n",
        "\n",
        "This notebook leverages [TorchVision's](https://pytorch.org/vision/stable/transforms.html) transforms for applying data preprocessing transformations including data augmentation. In addition, we also use the [`imgaug`](https://imgaug.readthedocs.io/) library for applying augmentation transforms that are compatible with input images as well as their depth maps. \n",
        "\n",
        "---\n",
        "\n",
        "Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those two parameters, then the rest of the notebook should run smoothly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kqDyT0Uy3YJF"
      },
      "outputs": [],
      "source": [
        "model_ckpt = \"vinvino02/glpn-nyu\" # pre-trained model from which to fine-tune\n",
        "batch_size = 24 # batch size for training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IsUw1iDY1SQ"
      },
      "source": [
        "Before we start, let's install the `transformers` and `imgaug` libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnmrcuRf_KEV",
        "outputId": "52653134-1b01-46d7-c16a-3c304e6da6c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.5 MB 4.8 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.6 MB 80.8 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163 kB 88.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers imgaug -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opnsFFkp3YJH"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "To be able to share your model with the community, there are a few more steps to follow.\n",
        "\n",
        "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271,
          "referenced_widgets": [
            "2563364b11fa44d186a3ac512cc978a1",
            "6379be7d43c44c8fa77a5ba3b8a391cc",
            "71344050607e480aa7286c0bec6edd5e",
            "056294c454c14f93bb60ba0c624fca62",
            "61dbdf6b2ff7436dbd490446b1c8c2a4",
            "0b80d7d8a8ad420ca06aa75bd4e1ebc0",
            "818263ad4aad4e9b86caa686bf407eab",
            "6444e86eb6bb4ee382b5a904458de254",
            "ab32daa7270241a5aabfa112021ab5e8",
            "ff1ba31a7ee04981adadfde5f681cf86",
            "82d05d07f8114adeabb6469e40ce55f3",
            "128caf849ea7402ea2669c34879854fc",
            "4a2fc716cc0f4d238eefb2079232a14b",
            "e88da62ff6754f348c470ca549b42a3f"
          ]
        },
        "id": "G68agu4Tk8pn",
        "outputId": "faabc0f4-f52e-47bf-fc0d-6ccb1bcf1fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login successful\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxiB2pzb3YJI"
      },
      "source": [
        "Then you need to install Git-LFS to upload your model checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oJ5v-qZsk-j9"
      },
      "outputs": [],
      "source": [
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUrtqwjq3YJJ"
      },
      "source": [
        "## Fine-tuning a model on a depth estimation task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXhLMD2q3YJK"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) vision models on a Depth Estimation dataset.\n",
        "\n",
        "Given a 2D image, the goal is to estimate the depths of the different objects present inside the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMiposSLY3m1"
      },
      "source": [
        "### Preparing the dataset\n",
        "\n",
        "We'll closely follow [this guide](https://keras.io/examples/vision/depth_estimation/) to prepare our dataset. First, we download the following:\n",
        "\n",
        "* A subset of the training split of the DIODE dataset.\n",
        "* The validation split of the DIODE dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "hf_dataset_identifier = \"sayakpaul/diode-subset-train\"\n",
        "filename = \"train_subset.tar.gz\"\n",
        "train_archive_path = hf_hub_download(\n",
        "    repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\"\n",
        ")"
      ],
      "metadata": {
        "id": "aepA3sKv4lrL",
        "outputId": "80fbcb47-66b8-4851-dd20-5efc269d1b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8d322cbf1e044585bcc58a9c198cc3dc",
            "e4bbf5f4b830468ab6205bf9702cfb45",
            "c9a625df7c6c49a7b2513296626fc13b",
            "36007c551fc44a22a446b355d98a96ca",
            "db0df14a25014c75848f5d0e2fdf11da",
            "d54bf14cb6ee47f28349009345610f0b",
            "5642d1d1c5814fc8800016752f5d9bbf",
            "053b1afc7ff3499ab1763f450bb74b34",
            "9709c4e7c7bb48e7a1ad1b3d4e9f0c06",
            "f6ca18f61c694f87a072b9b7b32adddb",
            "dad2f52f61884c9184e9db3313659b6b"
          ]
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/2.05G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d322cbf1e044585bcc58a9c198cc3dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "23IQB0SLFx4J"
      },
      "outputs": [],
      "source": [
        "from urllib import request\n",
        "\n",
        "url = \"http://diode-dataset.s3.amazonaws.com/val.tar.gz\"\n",
        "val_archive_path, _ = request.urlretrieve(url, url.split(\"/\")[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQAnIZKF3YJL"
      },
      "source": [
        "We then un-tar the archives. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LOUNzXeXKaWi"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "with tarfile.open(train_archive_path) as f:\n",
        "    f.extractall()\n",
        "\n",
        "with tarfile.open(val_archive_path) as f:\n",
        "    f.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-fiKXDr3YJL"
      },
      "source": [
        "We'll limit the tutorial to use images only from the `indoors`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHWvN5sTMSIJ",
        "outputId": "d93f9887-211a-4c7f-a69a-c11baf3a5e8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00000_00000_indoors_050_000.png',\n",
              " '00000_00000_indoors_060_040.png',\n",
              " '00000_00000_indoors_060_050.png',\n",
              " '00000_00000_indoors_070_000.png',\n",
              " '00000_00000_indoors_070_020.png',\n",
              " '00000_00000_indoors_070_050.png',\n",
              " '00000_00000_indoors_090_010.png',\n",
              " '00000_00000_indoors_100_000.png',\n",
              " '00000_00000_indoors_110_010.png',\n",
              " '00000_00000_indoors_110_020.png',\n",
              " '00000_00000_indoors_110_030.png',\n",
              " '00000_00000_indoors_120_010.png',\n",
              " '00000_00000_indoors_120_020.png',\n",
              " '00000_00000_indoors_120_050.png',\n",
              " '00000_00000_indoors_130_050.png']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "variant = \"indoors\"\n",
        "sorted(\n",
        "    os.listdir(os.path.join(\"train_subset\", variant, \"scene_00000\", \"scan_00000\"))\n",
        ")[:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ISdJF9nNXfd"
      },
      "source": [
        "`00021_00191_indoors_000_040` consider this to be the ID of an (image, depth map, mask) triplet. In this case:\n",
        "\n",
        "* `00021_00191_indoors_000_040.png` refers to the original image\n",
        "* `00021_00191_indoors_000_040_depth.npy` is the depth map\n",
        "* `00021_00191_indoors_000_040_depth_mask.npy` is the mask that needs to applied to the map\n",
        "\n",
        "Check out the [official dataset website](https://diode-dataset.org/) for more details.\n",
        "\n",
        "In the cell, we write a utility to prepare `pandas` dataframes from the image and depth map paths for easier navigation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "G8bA5f39Mhj3",
        "outputId": "4aba4b3b-e74e-4848-f77e-a3db4c6c09fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c31fc0cbb938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_subset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c31fc0cbb938>\u001b[0m in \u001b[0;36mprepare_data_df\u001b[0;34m(dataset_root_path, variant)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilelist\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_depth_mask.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     }\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     return arrays_to_mgr(\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ],
      "source": [
        "# Reference: https://keras.io/examples/vision/depth_estimation/#-preparing-the-dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def prepare_data_df(dataset_root_path, variant=\"indoors\"):\n",
        "    path = os.path.join(dataset_root_path, variant)\n",
        "\n",
        "    filelist = []\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            filelist.append(os.path.join(root, file))\n",
        "\n",
        "    filelist.sort()\n",
        "    data = {\n",
        "        \"image\": [x for x in filelist if x.endswith(\".png\")],\n",
        "        \"depth\": [x for x in filelist if x.endswith(\"_depth.npy\")],\n",
        "        \"mask\": [x for x in filelist if x.endswith(\"_depth_mask.npy\")],\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = prepare_data_df(\"train_subset\")\n",
        "val_df = prepare_data_df(\"val\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF48zZVLPygE"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmzJ_zWPZGf1"
      },
      "source": [
        "### Loading the model\n",
        "\n",
        "In the next cell, we initialize a depth estimation model. We also initialize the feature extractor associated to the model. This will come in handy during pushing the model checkpoints to the Hub. The feature extractor also contains information about how the dataset needs to be preprocessed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WhgpW-6fB7v"
      },
      "outputs": [],
      "source": [
        "from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\n",
        "\n",
        "\n",
        "feature_extractor = GLPNFeatureExtractor.from_pretrained(model_ckpt)\n",
        "model = GLPNForDepthEstimation.from_pretrained(model_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybs093yHZILw"
      },
      "source": [
        "### Preparing datasets for training and evaluation\n",
        "\n",
        "In the next cell, we define a `DIODEDataset` to prepare datasets for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsPfkaBoUx71"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "from imgaug import augmenters as iaa\n",
        "import imgaug as ia\n",
        "\n",
        "\n",
        "class DIODEDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, transformation_chain, input_only=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe: pandas dataframe containing image path, depth map, and validity mask\n",
        "            transformation_chain: composed chain of preprocessing transformations for data\n",
        "            input_only (optional, str or List[str]): to denote the preprocessing layers to be discarded\n",
        "                from `imgaug` transformation chain\n",
        "\n",
        "        References:\n",
        "            a. https://keras.io/examples/vision/depth_estimation/\n",
        "            b. https://github.com/fabioperez/pytorch-examples/blob/master/notebooks/PyTorch_Data_Augmentation_Image_Segmentation.ipynb\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.transformation_chain = transformation_chain\n",
        "        self.min_depth = 0.1\n",
        "        self.input_only = input_only\n",
        "        self.to_tensor = torchvision.transforms.ToTensor()\n",
        "\n",
        "    def _process_depth_map(self, depth_map: np.ndarray, mask: np.ndarray):\n",
        "        mask = mask > 0\n",
        "        max_depth = min(300, np.percentile(depth_map, 99))\n",
        "        depth_map = np.clip(depth_map, self.min_depth, max_depth)\n",
        "        depth_map = np.log(depth_map, where=mask)\n",
        "\n",
        "        depth_map = np.ma.masked_where(~mask, depth_map)\n",
        "\n",
        "        depth_map = np.clip(depth_map, 0.1, np.log(max_depth))\n",
        "        depth_map = np.expand_dims(depth_map, axis=2)\n",
        "        return depth_map\n",
        "\n",
        "    def _activator_masks(self, images, augmenter, parents, default):\n",
        "        if self.input_only and augmenter.name in self.input_only:\n",
        "            return False\n",
        "        else:\n",
        "            return default\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.dataframe.iloc[idx][\"image\"]\n",
        "        depth_map_path = self.dataframe.iloc[idx][\"depth\"]\n",
        "        mask_path = self.dataframe.iloc[idx][\"mask\"]\n",
        "\n",
        "        image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "        image = np.asarray(image)\n",
        "\n",
        "        depth_map = np.load(depth_map_path).squeeze()\n",
        "        mask = np.load(mask_path)\n",
        "        depth_map = self._process_depth_map(depth_map, mask)\n",
        "\n",
        "        if self.input_only:\n",
        "            det_tf = self.transformation_chain.to_deterministic()\n",
        "            image = det_tf.augment_image(image)\n",
        "            depth_map = det_tf.augment_image(\n",
        "                depth_map, hooks=ia.HooksImages(activator=self._activator_masks)\n",
        "            )\n",
        "            image = self.to_tensor(image)\n",
        "            depth_map = self.to_tensor(depth_map.copy())\n",
        "        else:\n",
        "            image = self.transformation_chain(image)\n",
        "            depth_map = self.transformation_chain(depth_map)\n",
        "        return {\"image\": image, \"depth_map\": depth_map}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufkVu0Mp3YJO"
      },
      "source": [
        "The `_activator_masks()` utility (courtesy of [this notebook]( https://github.com/fabioperez/pytorch-examples/blob/master/notebooks/PyTorch_Data_Augmentation_Image_Segmentation.ipynb)) ensures that certain augmentation transformations are only applied to the input image and NOT to the depth maps. \n",
        "\n",
        "Next, we define two separate transformation chains: one for training and another one for validation as well as evaluation. Since our training dataset is quite small, training with data augmentation can be crucial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZLpg_Yc5fzD"
      },
      "outputs": [],
      "source": [
        "# Heatmap transformations: https://imgaug.readthedocs.io/en/latest/source/examples_heatmaps.html\n",
        "\n",
        "resize_to = (512, 512)\n",
        "\n",
        "train_transform_chain = iaa.Sequential(\n",
        "    [\n",
        "        iaa.Resize(resize_to, interpolation=\"linear\"),\n",
        "        iaa.Fliplr(0.5),  # affects heatmaps\n",
        "        iaa.Sharpen((0.0, 1.0), name=\"sharpen\"),  # sharpen (only) image\n",
        "        iaa.Sometimes(\n",
        "            0.5, iaa.Affine(rotate=(-45, 45))\n",
        "        ),  # rotate by -45 to 45 degrees (affects heatmaps)\n",
        "        iaa.Sometimes(\n",
        "            0.5, iaa.ElasticTransformation(alpha=50, sigma=5)\n",
        "        ),  # apply water effect (affects heatmaps)\n",
        "    ],\n",
        "    random_order=True,\n",
        ")\n",
        "\n",
        "test_transformation_chain = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.ToPILImage(),\n",
        "        torchvision.transforms.Resize(resize_to),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cC4TVBur6dYN"
      },
      "outputs": [],
      "source": [
        "train_dataset = DIODEDataset(train_df, train_transform_chain, [\"sharpen\"])\n",
        "validation_dataset = DIODEDataset(val_df, test_transformation_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK6sbOoi3YJP"
      },
      "source": [
        "Notice how we specified that the `sharpen` transformation is an input-only transformation i.e., it should not be applied to the depth maps. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IopUr1bZKd8"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlRcM2iAkmRA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "\n",
        "def visualize_depth_map(samples, model=None):\n",
        "    # Reference: https://keras.io/examples/vision/depth_estimation/#visualizing-samples\n",
        "    input, target = samples[\"image\"], samples[\"depth_map\"]\n",
        "    cmap = plt.cm.jet\n",
        "    cmap.set_bad(color=\"black\")\n",
        "\n",
        "    if model:\n",
        "        inputs = {\"pixel_values\": input}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs).predicted_depth\n",
        "        fig, ax = plt.subplots(6, 3, figsize=(12, 20))\n",
        "        for i in range(6):\n",
        "            ax[i, 0].imshow(input[i].permute(1, 2, 0).numpy().astype(\"float32\"))\n",
        "            ax[i, 1].imshow(target[i].permute(1, 2, 0).numpy().squeeze(), cmap=cmap)\n",
        "            ax[i, 2].imshow(outputs[i].numpy().squeeze(), cmap=cmap)\n",
        "\n",
        "    else:\n",
        "        fig, ax = plt.subplots(6, 2, figsize=(12, 20))\n",
        "        for i in range(6):\n",
        "            ax[i, 0].imshow(input[i].permute(1, 2, 0).numpy().astype(\"float32\"))\n",
        "            ax[i, 1].imshow(target[i].permute(1, 2, 0).numpy().squeeze(), cmap=cmap)\n",
        "\n",
        "\n",
        "temp_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "visualize_samples = next(iter(temp_dataloader))\n",
        "visualize_depth_map(visualize_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4586mTo5ZMMk"
      },
      "source": [
        "We can also visualize the (random) model predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k1gA1UUdaYm"
      },
      "outputs": [],
      "source": [
        "visualize_depth_map(visualize_samples, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7LpOjJNZOpV"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "To instantiate a `Trainer`, we will need to define the training configuration and the evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model.\n",
        "\n",
        "Most of the training arguments are pretty self-explanatory, but one that is quite important here is `remove_unused_columns=False`. This one will drop any features not used by the model's call function. By default it's `True` because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in our case, we need the unused features ('image' in particular) in order to create 'pixel_values' (which is a mandatory key our model expects in its inputs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "serARuISkNWY"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "model_name = model_ckpt.split(\"/\")[-1]\n",
        "batch_size = 24\n",
        "num_epochs = 10\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-diode\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=2 * batch_size,\n",
        "    gradient_accumulation_steps=1,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=20,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    fp16=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8h3-YYz3YJQ"
      },
      "source": [
        "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay. Since the best model might not be the one at the end of training, we ask the `Trainer` to load the best model it saved at the end of training. By default, `Trainer` will load the best model w.r.t the `loss` quantity. However, we can also specify `metric_for_best_model` in the trainer argument if we want `Trainer` to use some other metric to be used during training.\n",
        "\n",
        "The last argument `push_to_hub` allows the Trainer to push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally with a name that is different from the name of the repository, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"nielsr/vit-finetuned-cifar10\"` or `\"huggingface/nielsr/vit-finetuned-cifar10\"`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp93qW2K3YJR"
      },
      "source": [
        "Next, we need to define a function for how to compute the metrics from the predictions. There are several metrics that we can report for depth estimation (refer to the [GLPN paper](https://arxiv.org/abs/2201.07436) for details). Here we'll use the [RMSE metric](https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIhFYhUVisND"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes RMSE on a batch of predictions\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    rmse = (labels - logits) ** 2\n",
        "    rmse = np.sqrt(rmse.mean())\n",
        "    return {\"rmse\": rmse}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q75j8BO03YJR"
      },
      "source": [
        "We also define a `collate_fn`, which will be used to batch examples together.\n",
        "Each batch consists of 2 keys, namely `pixel_values` and `labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8_BsEL_kAra"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"image\"] for example in examples])\n",
        "    labels = torch.stack([example[\"depth_map\"].squeeze() for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUwXSk9KZbb6"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Trainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmHl1uzflEAL"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=feature_extractor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCOkKocE3YJS"
      },
      "source": [
        "You might wonder why we pass along the `feature_extractor` as a tokenizer when we already preprocessed our data. This is only to make sure the feature extractor configuration file (stored as JSON) will also be uploaded to the repo on the hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J44HSz13YJS"
      },
      "source": [
        "Now we can finetune our model by calling the `train()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qZuqWkplSLU"
      },
      "outputs": [],
      "source": [
        "train_results = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjqLAAkhZeCr"
      },
      "source": [
        "We can now run evaluation with the `evaluate()` method of `Trainer`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsAHW0dLstug"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xGDSs6zZfmS"
      },
      "source": [
        "You can now upload the result of the training to the Hub, just execute this instruction (note that the Trainer will automatically create a model card as well as Tensorboard logs - see the \"Training metrics\" tab - amazing isn't it?):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo2R96fHwxAS"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"tags\": [\"vision\", \"depth-estimation\"],\n",
        "    \"finetuned_from\": model_ckpt,\n",
        "    \"dataset\": \"diode-subset\",\n",
        "}\n",
        "\n",
        "trainer.push_to_hub(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ryGv-VEZhq7"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now we can load the fine-tuned model and run inference with it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9XvMfGExzTM"
      },
      "outputs": [],
      "source": [
        "finetuned_model = GLPNForDepthEstimation.from_pretrained(\n",
        "    f\"sayakpaul/{model_name}-finetuned-diode\"\n",
        ")\n",
        "\n",
        "# We create a sample data loader with the tes set.\n",
        "temp_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=8)\n",
        "test_samples = next(iter(temp_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl-jfwLyM7-d"
      },
      "outputs": [],
      "source": [
        "visualize_depth_map(test_samples, finetuned_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz35Z5wL3YJT"
      },
      "source": [
        "## Pipeline API\n",
        "\n",
        "An alternative way to quickly perform inference with any model on the hub is by leveraging the [Pipeline API](https://huggingface.co/docs/transformers/main_classes/pipelines), which abstracts away all the steps we did manually above for us. It will perform the preprocessing, forward pass and postprocessing all in a single object. \n",
        "\n",
        "Let's showcase this for our trained model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkjEh3Xm3YJT"
      },
      "source": [
        "TODO"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2563364b11fa44d186a3ac512cc978a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6379be7d43c44c8fa77a5ba3b8a391cc",
              "IPY_MODEL_71344050607e480aa7286c0bec6edd5e",
              "IPY_MODEL_056294c454c14f93bb60ba0c624fca62",
              "IPY_MODEL_61dbdf6b2ff7436dbd490446b1c8c2a4"
            ],
            "layout": "IPY_MODEL_0b80d7d8a8ad420ca06aa75bd4e1ebc0"
          }
        },
        "6379be7d43c44c8fa77a5ba3b8a391cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_818263ad4aad4e9b86caa686bf407eab",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6444e86eb6bb4ee382b5a904458de254",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "71344050607e480aa7286c0bec6edd5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ab32daa7270241a5aabfa112021ab5e8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ff1ba31a7ee04981adadfde5f681cf86",
            "value": ""
          }
        },
        "056294c454c14f93bb60ba0c624fca62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_82d05d07f8114adeabb6469e40ce55f3",
            "style": "IPY_MODEL_128caf849ea7402ea2669c34879854fc",
            "tooltip": ""
          }
        },
        "61dbdf6b2ff7436dbd490446b1c8c2a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a2fc716cc0f4d238eefb2079232a14b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e88da62ff6754f348c470ca549b42a3f",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "0b80d7d8a8ad420ca06aa75bd4e1ebc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "818263ad4aad4e9b86caa686bf407eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6444e86eb6bb4ee382b5a904458de254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab32daa7270241a5aabfa112021ab5e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff1ba31a7ee04981adadfde5f681cf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82d05d07f8114adeabb6469e40ce55f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "128caf849ea7402ea2669c34879854fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "4a2fc716cc0f4d238eefb2079232a14b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88da62ff6754f348c470ca549b42a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d322cbf1e044585bcc58a9c198cc3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4bbf5f4b830468ab6205bf9702cfb45",
              "IPY_MODEL_c9a625df7c6c49a7b2513296626fc13b",
              "IPY_MODEL_36007c551fc44a22a446b355d98a96ca"
            ],
            "layout": "IPY_MODEL_db0df14a25014c75848f5d0e2fdf11da"
          }
        },
        "e4bbf5f4b830468ab6205bf9702cfb45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d54bf14cb6ee47f28349009345610f0b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5642d1d1c5814fc8800016752f5d9bbf",
            "value": "Downloading: 100%"
          }
        },
        "c9a625df7c6c49a7b2513296626fc13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_053b1afc7ff3499ab1763f450bb74b34",
            "max": 2047180800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9709c4e7c7bb48e7a1ad1b3d4e9f0c06",
            "value": 2047180800
          }
        },
        "36007c551fc44a22a446b355d98a96ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ca18f61c694f87a072b9b7b32adddb",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dad2f52f61884c9184e9db3313659b6b",
            "value": " 2.05G/2.05G [00:32&lt;00:00, 63.9MB/s]"
          }
        },
        "db0df14a25014c75848f5d0e2fdf11da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d54bf14cb6ee47f28349009345610f0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5642d1d1c5814fc8800016752f5d9bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "053b1afc7ff3499ab1763f450bb74b34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9709c4e7c7bb48e7a1ad1b3d4e9f0c06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6ca18f61c694f87a072b9b7b32adddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dad2f52f61884c9184e9db3313659b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}