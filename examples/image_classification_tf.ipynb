{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a58599",
   "metadata": {},
   "source": [
    "# **Fine-tuning for Image Classification with ðŸ¤— Transformers in TensorFlow**\n",
    "\n",
    "This notebook shows how to fine-tune any pretrained Vision model for Image Classification on a custom dataset. The idea is to add a randomly initialized classification head on top of a pre-trained encoder, and fine-tune the model altogether on a labeled dataset.\n",
    "\n",
    "## `tf.data` and `tensorflow_datasets`\n",
    "\n",
    "This notebook leverages [`tf.data`](https://www.tensorflow.org/guide/data_performance) and [`tensorflow_datasets`](https://www.tensorflow.org/datasets) (`tfds` for short) loading and preparing a custom dataset (namely, [`tf_flowers`](https://www.tensorflow.org/datasets/catalog/tf_flowers) in this tutorial). If you want to use a dataset that is not present in `tfds` then you can follow [this guide](https://www.tensorflow.org/tutorials/load_data/images). \n",
    "\n",
    "## Any model\n",
    "\n",
    "This notebook is built to run on any image classification dataset with any vision model checkpoint from the [Model Hub](https://huggingface.co/) as long as that model has a version with an image classification head, such as:\n",
    "\n",
    "* [TFViTForImageClassification](https://huggingface.co/docs/transformers/model_doc/vit#transformers.TFViTForImageClassification),\n",
    "* [TFSwinForImageClassification](https://huggingface.co/docs/transformers/main/en/model_doc/swin#transformers.TFSwinForImageClassification),\n",
    "* [TFConvNextForImageClassification](https://huggingface.co/docs/transformers/model_doc/convnext#transformers.TFConvNextForImageClassification) \n",
    "\n",
    "In short, any model supported by [TFAutoModelForImageClassification](https://huggingface.co/docs/transformers/model_doc/auto#transformers.TFAutoModelForImageClassification).\n",
    "\n",
    "\n",
    "## Data augmentation\n",
    "\n",
    "This notebook leverages the ops offered by the [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image/) module for data augmentation. We will soon be adding data augmentation support with libraries like [`albumentations`](https://albumentations.ai/).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Depending on the model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those two parameters, then the rest of the notebook should run smoothly.\n",
    "\n",
    "In this notebook, we'll fine-tune from the [`vit-base-patch16-224-in21k`](https://huggingface.co/google/vit-base-patch16-224-in21k) checkpoint, but note that there are many more available on the hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/vit-base-patch16-224-in21k\" # pre-trained model from which to fine-tune\n",
    "batch_size = 32 # batch size for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa731b",
   "metadata": {},
   "source": [
    "Before we start, let's install the `transformers` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd00647",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b476d34",
   "metadata": {},
   "source": [
    "If you're executing this notebook from [Google Colab](https://colab.research.google.com/) then you won't need to install TensorFlow and TensorFlow Datasets. Otherwise, you can install them with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07670056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5726d",
   "metadata": {},
   "source": [
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920fd1d",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS to upload your model checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc775b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!sudo apt -qq install git-lfs\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a728fde",
   "metadata": {},
   "source": [
    "## Fine-tuning on an image classification task\n",
    "\n",
    "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) vision models on an Image Classification dataset.\n",
    "\n",
    "Given an image, the goal is to predict an appropriate class for it, like \"tiger\". The screenshot below is taken from a [ViT fine-tuned on ImageNet-1k](https://huggingface.co/google/vit-base-patch16-224) - try out the inference widget!\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/tiger_image.png\" alt=\"drawing\" width=\"600\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf5b4a0",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "As mentioned above, we'll use `tfds` to load our dataset. In the below code cells, we first define a few useful constants, then we import the libraries, and finally we load the `tf_flowers` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70eb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "image_resolution = (224, 224, 3)\n",
    "\n",
    "# Dataset\n",
    "classes = [\n",
    "    \"dandelion\",\n",
    "    \"daisy\",\n",
    "    \"tulips\",\n",
    "    \"sunflowers\",\n",
    "    \"roses\",\n",
    "]  # don't change the order\n",
    "\n",
    "# Other constants\n",
    "mean = tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])  # imagenet mean\n",
    "std = tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])  # imagenet std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32871f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\"],\n",
    "    as_supervised=True,\n",
    "    try_gcs=False,  # gcs_path is necessary for tpu,\n",
    ")\n",
    "\n",
    "num_train = tf.data.experimental.cardinality(train_dataset)\n",
    "num_val = tf.data.experimental.cardinality(val_dataset)\n",
    "print(f\"Number of training examples: {num_train}\")\n",
    "print(f\"Number of validation examples: {num_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ee955",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "In this section, we'll prepare the dataset so that it can be used for training. In a nutshell, this is what we are going to do:\n",
    "\n",
    "* Resize the images to have a uniform resolution (224x224x3 in our case).\n",
    "* Apply augmentation operations (like random flipping, cropping, etc.) to the training set.\n",
    "* Normalize the images.\n",
    "* Batch the images and labels.\n",
    "\n",
    "We'll write a utility function for doing these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(\n",
    "    dataset: tf.data.Dataset, train: bool, image_resolution=image_resolution\n",
    "):\n",
    "    def preprocess(image, label):\n",
    "        # For training, do augmentation.\n",
    "        if train:\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_flip_up_down(image)\n",
    "            image = tf.image.random_crop(image, size=image_resolution)\n",
    "        else:\n",
    "            image = tf.image.resize(\n",
    "                image, size=(image_resolution[0], image_resolution[1])\n",
    "            )\n",
    "        image = (image - MEAN) / STD  # Normalization.\n",
    "        return image, label\n",
    "\n",
    "    # Shuffle the training images.\n",
    "    if train:\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
    "\n",
    "    dataset = dataset.map(preprocess, AUTO).batch(BATCH_SIZE)\n",
    "    # Transpose because the `transformers` model has a leading channel dimension.\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: (tf.transpose(x, [0, 3, 1, 2]), y), tf.data.AUTOTUNE\n",
    "    )\n",
    "    # We apply prefetching to prepare a couple of batches async on CPU\n",
    "    # so that our model doesn't starve for data.\n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a407b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_dataset, True)\n",
    "val_dataset = make_dataset(val_dataset, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273a73c",
   "metadata": {},
   "source": [
    "Let's visualize a couple of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd016a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images, sample_labels = next(iter(train_dataset))\n",
    "\n",
    "plt.figure(figsize=(5 * 3, 3 * 3))\n",
    "for n in range(15):\n",
    "    ax = plt.subplot(3, 5, n + 1)\n",
    "    image = (sample_images[n].numpy().transpose(1, 2, 0) * std + mean).numpy()\n",
    "    image = (image - image.min()) / (\n",
    "        image.max() - image.min()\n",
    "    )  # Convert to [0, 1] for avoiding matplotlib warning.\n",
    "    plt.imshow(image)\n",
    "    plt.title(classes[sample_labels[n]])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6405e6",
   "metadata": {},
   "source": [
    "### Preparing the model\n",
    "\n",
    "Now that our data is ready, we can download the pretrained model and fine-tune it. For classification we use the `TFAutoModelForImageClassification` class. The `from_pretrained` method will download and cache the model for us. As the label ids and the number of labels are dataset dependent, we pass `num_labels` alongside the `model_checkpoint` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c09e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "num_labels = len(classes)\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=num_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af41535",
   "metadata": {},
   "source": [
    "If you need more explicit controls you can also define your model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d733e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TFViTModel\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow import keras\n",
    "\n",
    "\n",
    "# base_model = model = TFViTModel.from_pretrained(model_id)\n",
    "# base_model.trainable = True\n",
    "\n",
    "# inputs = layers.Input((3, image_size[0], image_size[1]))\n",
    "# x = base_model(inputs, training=False)\n",
    "# outputs = layers.Dense(len(classes))(x.last_hidden_state[:, 0])\n",
    "# model = keras.Model(inputs, outputs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39320708",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84237d78",
   "metadata": {},
   "source": [
    "Let's now initialize our optimizer and learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68126ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 3\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01\n",
    "num_warmup_steps = 0\n",
    "\n",
    "\n",
    "num_train_steps = (num_train // batch_size) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb7facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff2829",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=num_train_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c1128",
   "metadata": {},
   "source": [
    "## Push to model hub\n",
    "\n",
    "You can now upload trained model to the Hub. Just execute this instruction below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0831c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import push_to_hub_keras, from_pretrained_keras\n",
    "\n",
    "# Push to HuggingFace Hub\n",
    "push_to_hub_keras(model, \"sayakpaul/vit-finetuned-flowers-tf\")\n",
    "\n",
    "# Reload from HuggingFace Hub\n",
    "reloaded = from_pretrained_keras(\"sayakpaul/vit-finetuned-flowers-tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b2f6d",
   "metadata": {},
   "source": [
    "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```py\n",
    "from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\"sayakpaul/my-awesome-model\")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
